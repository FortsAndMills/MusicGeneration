# Почему всё так сложно?

В попытках понять, почему так сложно придумать *запоминающую* модель, приходится задумываться о том, а те ли в принципе закономерности предлагаемыми моделями могут быть уловлены?

Вот, например, [забавный эксперимент](https://github.com/FortsAndMills/MusicGeneration/blob/master/two%20neurals.ipynb). Для нас закономерность ABABAB... одна из самых простых. Но обычные нейронки так не считают... В эксперименте одна нейронка (Q-агент) каждый ход нажимает одну клавишу из тринадцати так, чтобы скайлёрновский MLPClassifier угадывал это, но неидеально. Если угадывание идеально, то штраф начинает экспоненциально расти, выпинывая товарищей из ямы. Пока иногда сходится к выдаче одной и той же клавиши, но всё это кривоватенько выглядит и непохоже, что они выберутся из этого локального оптимума и тем более придумают ABABAB.

Конечно, машинное обучение уже придумало, как учить модель считать. Классика - LSTM, но есть и более специфичные варианты, подробнее вот [в этом разделе](https://github.com/FortsAndMills/MusicGeneration/tree/master/%D0%9F%D0%BE%D0%B4%D1%85%D0%BE%D0%B4%D1%8B%20%D0%BA%20%D0%B3%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8%20%D0%BC%D1%83%D0%B7%D1%8B%D0%BA%D0%B8#А-что-ещё-тут-не-пробовали). 

Из пощупанного - GAN на мнисте, после подмены данных простой эксперимент в лоб выдаёт быструю сходимость к одной из песен, что с одной стороны понятно (нейросеть + мало данных), а с другой интересно, почему проходы по другим песням не вышибают сеть из локального оптимума.

Ещё [одна затея](https://github.com/FortsAndMills/MusicGeneration/blob/master/Autoencoder%20XtoX%20transformations%20test.ipynb): будем считать, что есть набор хороших примеров, и одни примеры можно получать из других некоторыми преобразованиями. Хочется найти преобразование. Вообще, хочется найти такую функцию f, что f(X_i) = X_j для некоторого j != i. Не будем думать о слонах и попробуем сформулировать это в терминах задачи для непрерывной оптимизации... А f приблизим нейронкой (под рукой была топология автокодировщика). Пока ерунда, f сходится к константе, по функционалу равномерно удаленной от всех объектов выборки.

Последние два эксперимента закончились схлопыванием генератора к константе. Оказывается, на больших датасетах такое вполне себе тоже часто встречается и является повсеместной проблемой. Решением якобы является довольно вставная и тяжёлая вещь, т.н. Minibatch discrimination...

# Двигаеясь в направлении LSTM-а...
Ну окей-окей, раз слонами задачу не решить, и нужно действовать по-научному, то LSTM-таки нужно достать, и делать им одно из двух: или пытаться строить предикат музыки (то есть "слушателя" в терминах идей выше), или, по классике, генератор. Попробовал на, кхм, 35 песенках первое; батч составляется так: с высокой вероятностью берётся неизменное исходное произведение с таргетом 1, а с экспоненциально падающей в него добавляются "ошибки" - случайные подмены нот и пр., что определённо всё ломает. Таргет при этом соответственно падает, и задачей LSTM-а по сути становится задача регрессии, насколько именно данные "испорчены". Естественно, lstm обучается, а дальше можно делать а) пытаться из шума градиентным спуском сойтись к чему-то, у чего будет высокий отклик сети (>> 1...), минусы - ноты становятся непрерывными, но казалось бы это самый грамотный подход; б) генетический пооооиск! Плюсы - ноты всё-таки будут 0 и 1. Сходится к вариантам с кучей единиц или одним нулям, т.е. вещам, которых в выборке в помини не было. Видимо, надо добавить в выборку нагенерированных примеров от балды.

## Вставка памяти для поиска повторов
LSTM действительно не воспринимает такую закономерность, как повтор, и это ключевая проблема. Отсюда идея, давайте добавим в сеть внешнюю память, HistoryUser (см. ). Есть менее костыльная, но технически сложная в реализации затея следующего вида. Нужно научить модель повтору. Значит, нужно иметь нечто, что будет говорить нейросети, что знаешь, ты уже была вот в том "состоянии", в котором ты сейчас находишься, и правильный ответ тогда был вот такой. Другая интерпретация - память модели есть ещё одна переобученная модель. Например, 1NN. Здесь есть некие параллели с тьюринг-нейронкой, и по этой затеи есть ещё комментарии, но с реализацией сложности; в том числе, скорее всего придётся обучать её эволюционно.
