## Глобальная задача

[Подходы к генерации музыки](https://github.com/FortsAndMills/MusicGeneration/tree/master/%D0%9F%D0%BE%D0%B4%D1%85%D0%BE%D0%B4%D1%8B%20%D0%BA%20%D0%B3%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8%20%D0%BC%D1%83%D0%B7%D1%8B%D0%BA%D0%B8) - сборник ссылочек по этой теме. Нас конкретнее интересует именно генерация midi-файла.

## Работа с midi в питоне
Существует много библиотек под питон, я использую [mido](https://mido.readthedocs.io/en/latest/).

Во всех экспериментах используется songs.py - простенький модуль, который пока может:
* загружать миди-файлы
* проверять их на то, что ноты не выходят за диапазон
* проигрывать их прямо в ноутбуке
* переводить в булев массив [время x количество клавиш]
* создавать новый миди-файл по передаваемым строкам булева массива
* вносить в ноты "ошибки" примитивным алгоритмом

## Данные

Для примитивных тестов - [34 песенки](https://github.com/FortsAndMills/MusicGeneration/tree/master/basic%20midi) из сборника произведений для первоклашек.

В интернете гуляет много сборников миди, в т.ч. больших объёмов, однако с данными легко напороться на проблему:
* MIDI по-хорошему должны быть именно для фортепианного произведения. Часто попадается что-то не то.
* Данные могут быть записаны "вживую", то есть ноты не попадают на разметку по долям. Самый тяжёлый случай - когда на разметку вообще забили, просто записали живую запись, темп указали дефолтный, в результате при дискретизации получается полная ересь.
* Переменный темп произведения - казалось бы не так страшно, если ноты вбиты по сетке (тогда факт смены темпа как бы просто игнорируется). Однако, постоянно напарывался на то, будто сетку подстраивали под "живую запись" автоподбором темпа, в результате чего темп меняется каждую божью наносекунду. Ноты "получаются" по сетке, конечно, но сама сетка просто-напросто деформирована. Толку от этого ноль, восстановить дискретизацию в таких случаях не получается. Фильтровать такие случае получается тоже плохо.
* Переменный ритм. Для читерского использования зависимости "а 16 тактами ранее мы сыграли..." хотелось бы оставить только 4/4 и аналоги.
* Фортепианные произведения "известных классических композиторов" - это грань человеческого понимания. Однако в датасетах обычно именно они, и получается, что нейросеть сразу учится самым нетривиальным паттернам.  

# Упрощённая задача

Пока возьмём только 13 клавиш (одна октава + верхняя до, в стандартных обозначениях - C, C#, D, D#, E, F, F#, G, G#, A, A#, B и ещё одна C). В каждый момент времени клавиша либо нажимается, либо отпущена (т.е. понятия зажатой клавиши нет).

Также пока предположим отсутствие реккурентности; вместо памяти будем подавать на вход не только текущий, но и предыдущие моменты времени (скажем, 4 такта; вроде оптимально, кстати, будет брать 1, 2, 4, 8 и т.д. до некоторого предела).

### Переобучение vs запоминание

Первоначальная идея была реализовать обучение с подкреплением (нейросеть на кучи данных, выдающая reward ("слушатель") + "исполнитель", обучающийся на этом reward-е через Q-обучение или нейроэволюцию). Сильно упростим эту задачу: хотим всего лишь научить исполнителя играть кузнечика.

- если алгоритм детерминирован, то на входе, совпадающий с началом выученной песенки, он в точности повторит её без вероятности придумать другое продолжение. Есть способы обойти это - дополнительный вход, равный 1 при изучении песни, который при желании "импровизировать" выключается.
- а вот на новом входе алгоритм теоретически может выдать что-то ещё. Если запоминание обучающей выборки происходит каким-то образом с выделением закономерностей и знаний, то так может и что-то получится на выходе.
- в конце концов, если мы не можем решить эту задачу для "исполнителя" ("нажми вот эти кнопки в заданной последовательности и никак иначе"), то что уж говорить о задаче "а ну, играй на 13 кнопках то, что слушатель посчитает музыкой".

## NEAT

[Нейроэволюционный подход](https://github.com/FortsAndMills/MusicGeneration/tree/master/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D1%8D%D0%B2%D0%BE%D0%BB%D1%8E%D1%86%D0%B8%D1%8F) хорош тем, что якобы умеет учиться нажимать кнопки (в данном случае - клавиши) в правильной последовательности. Однако пока что заставить его построить нейросеть, которая запомнит даже первые несколько тактов, [не удалось](https://github.com/FortsAndMills/MusicGeneration/tree/master/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D1%8D%D0%B2%D0%BE%D0%BB%D1%8E%D1%86%D0%B8%D1%8F#ЭКСПЕРИМЕНТЫ) :( Возможно, я криворукий. 

Поэтому исследование ВНЕЗАПНО приняло неожиданный оборот:

## Булевоалгебраический подход

Тут уже нужен TeX, поэтому подробное полное описание и все идеи занесены в [общую презентацию](https://github.com/FortsAndMills/MusicGeneration/blob/master/Discon/%D0%A1%D0%BE%D0%B1%D1%80%D0%B0%D0%BD%D0%B8%D0%B5%20%D1%81%D0%BE%D1%87%D0%B8%D0%BD%D0%B5%D0%BD%D0%B8%D0%B9...%20%D0%B3%D0%BC%2C%20%D0%B1%D1%83%D0%BB%D0%B5%D0%B2%D1%8B%D1%85.pdf).

Для тех, кому лень смотреть презентацию, вот лаконичная интерпретация идеи от А.Думбая (2017):

![alt text](https://github.com/FortsAndMills/MusicGeneration/blob/master/Discon/Opinion.gif)

## Нейросети слона-суицидника

Происходящее [в этом разделе](https://github.com/FortsAndMills/MusicGeneration/tree/master/Elephant%20Neural%20Nets) отлично отражено в последнем слове названия... Но хочется научиться строить типа-нейросеть, выдающую правильный ответ на заданных объектах. Это в некотором смысле обобщение подхода с булевыми функциями.

Глобальная проблема подхода - непонятно, как выбирать действие так, чтобы равномерно случайно спускаться вглубь сети. В целом эту проблему решить удалось (вводим качество связей -> уверенность сигнала нейрона оцениваем как минимальное произведение качеств связей на пути до него -> это примерно экспоненциальное падение уверенности -> с конца шагаем к началу с вероятностями "уверенности сигнала"), а вот следующий глобальный шаг - сбалансировать рост и удаление нейронов - пока не удался.

## Пытаемся не думать о слонах...

Прочие неудачные эксперименты с базовым датасетом из, кхм, 34 песенок, были вынесены в [отдельную папку](https://github.com/FortsAndMills/MusicGeneration/tree/master/%D0%9F%D0%BE%D0%B4%D1%85%D0%BE%D0%B4%D1%8B%20%D0%BA%20%D0%B3%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8%20%D0%BC%D1%83%D0%B7%D1%8B%D0%BA%D0%B8).

## HistoryUser
Базовые эксперименты модели - [здесь](https://github.com/FortsAndMills/MusicGeneration/tree/master/HistoryUser%20basic%20tests).

Идея следующая: очередной вход последовательности проходит через LSTM, преобразуются к вектору размерности, скажем, 80 и нормируются соофтмаксом. Это - веса для считывания из "истории", то есть предыдущих 80 входов. Это и есть "модель памяти", или точнее - истории. Его выход дальше нужно обрабатывать - по идее, ещё LSTM-ом. Проведены тесты, подтверждающие, что такая модель способна обучаться в принципе, то есть градиенты сводят на считывание откуда надо.

# Real Data, Real Challenge!

В связи с зачётной сессией была поставлена задача ~~сдать её~~ сделать что-нибудь, лишь бы что-нибудь выдавало. Без сомнений, в такой формулировке задача была решена. Подробное описание модели [здесь](https://github.com/FortsAndMills/MusicGeneration/tree/master/RealData%20First%20Model).

Итак...
Для получения ИИ нужно решить следующие проблемы:
1) С данными полная лажа. Имеющийся и использованный (!) в первой модели обработчик датасетов от анонимусов, при помощи которого получен т.н. CM_dataset, крив чуть более чем полностью. Получившийся датасет из 100+ песенок, вроде как более-менее, в том плане, что при прослушивании песенок из него нет ощущения полной лажи, то есть по крайней мере дискретизация прошла нормально. Да и сами песенки не из разряда Рахманинова, то есть выделить там паттерны человеческим языком можно, поэтому вариант не самый худший, но может можно б найти и чего получше?..
2) Память. Это вроде как самое интересное, поскольку музыка построена в первую очередь на закономерностях вида "повторы"-"чередования"-"тоже самое, только циклически сдвинуть". И моделей для этих закономерностей - нет (параметрических, по крайней мере). В первой модели используется работающий HistoryUser, а также проводились в целом не разочаровывающие эксперименты с "дообучением" по ходу генерации.
3) Полифоническое сэмплирование. Эту проблему я недооценил. На выходе нейросети в каждый момент времени должно быть подмножество (!) нот. И вычисление одних элементов этого множества необратимо влияет на распределение на остальных потенциальных элементах! Вот эту проблему умные люди *как-то* решают, см. [про RBM здесь](https://github.com/FortsAndMills/MusicGeneration/blob/master/%D0%9F%D0%BE%D0%B4%D1%85%D0%BE%D0%B4%D1%8B%20%D0%BA%20%D0%B3%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8%20%D0%BC%D1%83%D0%B7%D1%8B%D0%BA%D0%B8/Readme.md#rbm-%D0%9A%D0%B0%D0%BA%D0%BE%D0%B9-%D0%B5%D1%89%D1%91-rbm). В первой модели в конечном итоге использовался вариант "сэмплирования по голосу": по выходу нейросети маленький однослойный LSTM генерил первую ноту, затем другой маленький LSTM вторую ноту, и т.д. пять нот. С обучением такой модели (известен ж только "итоговый" набор из пяти нот) есть нюансы, но гармонии уловить модель смогла, а вот строить интерпретируемые голоса не научилась. 
4) Оптимизация. Даже на 100-300 песенках обучение длится сутки и более. :о( В первой модели использовалась костыльная и странная, но работающая хитрость: произведения сжимались в одну октаву, и работа велась с 12-мерными векторами, а не 88-мерными. Я верил, что расшифровка (обратное преобразование) будет не такой сложной задачей, но, естественно, она не без потерь. Тем не менее, ошибки при таком подходе "на октаву", что при субъективном прослушивании не так заметно.

## Далее в программе:

* попробованы варианты с другими датасетами. Существенных изменений нет; при обучении на TheGreats, сборнике классики, механизм внимания размывается.
* добавка History(t - 1) как инпут для контроллера получше разрезило распределение на считывание; сильно ситуацию не улучшило
* при этом дополнительные связи у 12 нейронов контроллера с аггрегирующим слоем, кажется, только дело портят, но непонятно.

Совсем альтернативный вариант:
* иерархический лстм. Непонятно, как обучать это в принципе + неясно, как формировать выход, а похоже, больше проблем даже со вторым. Пока ставим галочку в форме вопросительного знака.

Ещё вариант: сведение к задаче классификации на 4096 (на самом деле меньше) вида. Итак:
* на базовом датасете привело к хорошим результатам! Явно было заметно использование выхода из модуля истории.
* на бОльших датасетах обучаться "не хочет" --- обучается плохо с соотв. результатом. Интересно, почему; контроллер опять размывает.
* по идее, можно в рамках "4096 возможных исходов" считать вероятности, исходя из результатов поголосового сэмплирования. Однако, для этого нужны хитрости с линалом. Итак, у нас есть батч размера b последовательностей размера t, для каждого момента генерируется матрица 5x13 вероятностей засэмплировать одну из 13 нот для каждого из 5 голосов; нужно перевести это в 4096 вероятностей всех возможных исходов. Для этого нужно перебирать много комбинаций; я нашёл фокус, позволяющий перебирать всего 32 комбинации для получения одной из 4096 вероятностей. Однако, матрица b x t x 32 x 13 x 4096 уже ни в какую память не влезает. Тем не менее, можно посчитать одну вероятность, b x t x 32 x 13, и таким образом считать правдоподобие в терминах классификации. ML, однако, не взлетел. Также не помогла приблизительная оценка кросс-энтропии (для которой, в теории, нужны все 4096 комбинации) сэмплированием для каждого батча случайных 100 комбинаций из 4096, для которых будет считаться 1 - вероятность её генерации; увы, аналогично.


