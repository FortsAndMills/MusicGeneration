# !
Репозиторий в состоянии большого апдейта и структуризации информации. Будьте осторожны!

## Глобальная задача

[Подходы к генерации музыки](https://github.com/FortsAndMills/MusicGeneration/tree/master/%D0%9F%D0%BE%D0%B4%D1%85%D0%BE%D0%B4%D1%8B%20%D0%BA%20%D0%B3%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8%20%D0%BC%D1%83%D0%B7%D1%8B%D0%BA%D0%B8) - сборник ссылочек по этой теме. Нас конкретнее интересует именно генерация midi-файла.

## Работа с midi в питоне
Существует много библиотек под питон, я использую [mido](https://mido.readthedocs.io/en/latest/).

Во всех экспериментах используется songs.py - простенький модуль, который пока может:
* загружать миди-файлы
* проверять их на то, что ноты не выходят за диапазон
* проигрывать их прямо в ноутбуке
* переводить в булев массив [время x количество клавиш]
* создавать новый миди-файл по передаваемым строкам булева массива
* вносить в ноты "ошибки" примитивным алгоритмом

## Данные

Для примитивных тестов - [34 песенки](https://github.com/FortsAndMills/MusicGeneration/tree/master/basic%20midi) из сборника произведений для первоклашек.

В интернете гуляет много сборников миди, в т.ч. больших объёмов, однако с данными легко напороться на проблему:
* MIDI по-хорошему должны быть именно для фортепианного произведения. Часто попадается что-то не то.
* Данные могут быть записаны "вживую", то есть ноты не попадают на разметку по долям. Самый тяжёлый случай - когда на разметку вообще забили, просто записали живую запись, темп указали дефолтный, в результате при дискретизации получается полная ересь.
* Переменный темп произведения - казалось бы не так страшно, если ноты вбиты по сетке (тогда факт смены темпа как бы просто игнорируется). Однако, постоянно напарывался на то, будто сетку подстраивали под "живую запись" автоподбором темпа, в результате чего темп меняется каждую божью наносекунду. Ноты "получаются" по сетке, конечно, но сама сетка просто-напросто деформирована. Толку от этого ноль, восстановить дискретизацию в таких случаях не получается. Фильтровать такие случае получается тоже плохо.
* Переменный ритм. Для читерского использования зависимости "а 16 тактами ранее мы сыграли..." хотелось бы оставить только 4/4 и аналоги.
* Фортепианные произведения "известных классических композиторов" - это грань человеческого понимания. Однако в датасетах обычно именно они, и получается, что нейросеть сразу учится самым нетривиальным паттернам.  

# Упрощённая задача

Пока возьмём только 13 клавиш (одна октава + верхняя до, в стандартных обозначениях - C, C#, D, D#, E, F, F#, G, G#, A, A#, B и ещё одна C). В каждый момент времени клавиша либо нажимается, либо отпущена (т.е. понятия зажатой клавиши нет).

Также пока предположим отсутствие реккурентности; вместо памяти будем подавать на вход не только текущий, но и предыдущие моменты времени (скажем, 4 такта; вроде оптимально, кстати, будет брать 1, 2, 4, 8 и т.д. до некоторого предела).

## Переобучение vs запоминание

Первоначальная идея была реализовать обучение с подкреплением (нейросеть на кучи данных, выдающая reward ("слушатель") + "исполнитель", обучающийся на этом reward-е через Q-обучение или нейроэволюцию). Сильно упростим эту задачу: хотим всего лишь научить исполнителя играть кузнечика.

- если алгоритм детерминирован, то на входе, совпадающий с началом выученной песенки, он в точности повторит её без вероятности придумать другое продолжение. Есть способы обойти это - дополнительный вход, равный 1 при изучении песни, который при желании "импровизировать" выключается.
- а вот на новом входе алгоритм теоретически может выдать что-то ещё. Если запоминание обучающей выборки происходит каким-то образом с выделением закономерностей и знаний, то так может и что-то получится на выходе.
- в конце концов, если мы не можем решить эту задачу для "исполнителя" ("нажми вот эти кнопки в заданной последовательности и никак иначе"), то что уж говорить о задаче "а ну, играй на 13 кнопках то, что слушатель посчитает музыкой".

# NEAT

[Нейроэволюционный подход](https://github.com/FortsAndMills/MusicGeneration/tree/master/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D1%8D%D0%B2%D0%BE%D0%BB%D1%8E%D1%86%D0%B8%D1%8F) хорош тем, что якобы умеет учиться нажимать кнопки (в данном случае - клавиши) в правильной последовательности. Однако пока что заставить его построить нейросеть, которая запомнит даже первые несколько тактов, [не удалось](https://github.com/FortsAndMills/MusicGeneration/tree/master/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D1%8D%D0%B2%D0%BE%D0%BB%D1%8E%D1%86%D0%B8%D1%8F#ЭКСПЕРИМЕНТЫ) :( Возможно, я криворукий. 

Поэтому исследование ВНЕЗАПНО приняло неожиданный оборот:

# Булевоалгебраический подход

Тут уже нужен TeX, поэтому подробное полное описание и все идеи занесены в [общую презентацию](https://github.com/FortsAndMills/MusicGeneration/blob/master/Discon/%D0%A1%D0%BE%D0%B1%D1%80%D0%B0%D0%BD%D0%B8%D0%B5%20%D1%81%D0%BE%D1%87%D0%B8%D0%BD%D0%B5%D0%BD%D0%B8%D0%B9...%20%D0%B3%D0%BC%2C%20%D0%B1%D1%83%D0%BB%D0%B5%D0%B2%D1%8B%D1%85.pdf).

Для тех, кому лень смотреть презентацию, вот лаконичная интерпретация идеи от А.Думбая (2017):

![alt text](https://github.com/FortsAndMills/MusicGeneration/blob/master/Discon/Opinion.gif)

# Нейросети слона-суицидника

Происходящее [в этом разделе](https://github.com/FortsAndMills/MusicGeneration/tree/master/Elephant%20Neural%20Nets) отлично отражено в последнем слове названия... Но хочется научиться строить типа-нейросеть, выдающую правильный ответ на заданных объектах. Это в некотором смысле обобщение подхода с булевыми функциями.

Глобальная проблема подхода - непонятно, как выбирать действие так, чтобы равномерно случайно спускаться вглубь сети. В целом эту проблему решить удалось (вводим качество связей -> уверенность сигнала нейрона оцениваем как минимальное произведение качеств связей на пути до него -> это примерно экспоненциальное падение уверенности -> с конца шагаем к началу с вероятностями "уверенности сигнала"), а вот следующий глобальный шаг - сбалансировать рост и удаление нейронов - пока не удался.

# Пытаемся не думать о слонах

[Забавный эксперимент](https://github.com/FortsAndMills/MusicGeneration/blob/master/two%20neurals.ipynb). Для нас закономерность ABABAB... одна из самых простых. Но обычные нейронки так не считают... В эксперименте одна нейронка (Q-агент) каждый ход нажимает одну клавишу из тринадцати так, чтобы скайлёрновский MLPClassifier угадывал это, но неидеально. Если угадывание идеально, то штраф начинает экспоненциально расти, выпинывая товарищей из ямы. Пока иногда сходится к выдаче одной и той же клавиши, но всё это кривоватенько выглядит и непохоже, что они выберутся из этого локального оптимума и тем более придумают ABABAB.

Конечно, машинное обучение уже придумало, как учить модель считать. Классика - LSTM, но есть и более специфичные варианты, подробнее вот [в этом разделе](https://github.com/FortsAndMills/MusicGeneration/tree/master/%D0%9F%D0%BE%D0%B4%D1%85%D0%BE%D0%B4%D1%8B%20%D0%BA%20%D0%B3%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8%20%D0%BC%D1%83%D0%B7%D1%8B%D0%BA%D0%B8#А-что-ещё-тут-не-пробовали). 

Из пощупанного - GAN на мнисте, после подмены данных простой эксперимент в лоб выдаёт быструю сходимость к одной из песен, что с одной стороны понятно (нейросеть + мало данных), а с другой интересно, почему проходы по другим песням не вышибают сеть из локального оптимума.

Ещё [одна затея](https://github.com/FortsAndMills/MusicGeneration/blob/master/Autoencoder%20XtoX%20transformations%20test.ipynb): будем считать, что есть набор хороших примеров, и одни примеры можно получать из других некоторыми преобразованиями. Хочется найти преобразование. Вообще, хочется найти такую функцию f, что f(X_i) = X_j для некоторого j != i. Не будем думать о слонах и попробуем сформулировать это в терминах задачи для непрерывной оптимизации... А f приблизим нейронкой (под рукой была топология автокодировщика). Пока ерунда, f сходится к константе, по функционалу равномерно удаленной от всех объектов выборки.

Последние два эксперимента закончились схлопыванием генератора к константе. Оказывается, на больших датасетах такое вполне себе тоже часто встречается и является повсеместной проблемой. Решением якобы является довольно вставная и тяжёлая вещь, т.н. Minibatch discrimination...

# Двигаемся в направлении LSTM-а
Ну окей-окей, раз слонами задачу не решить, и нужно действовать по-научному, то LSTM-таки нужно достать, и делать им одно из двух: или пытаться строить предикат музыки (то есть "слушателя" в терминах идей выше), или, по классике, генератор. Попробовал на, кхм, 35 песенках первое; батч составляется так: с высокой вероятностью берётся неизменное исходное произведение с таргетом 1, а с экспоненциально падающей в него добавляются "ошибки" - случайные подмены нот и пр., что определённо всё ломает. Таргет при этом соответственно падает, и задачей LSTM-а по сути становится задача регрессии, насколько именно данные "испорчены". Естественно, lstm обучается, а дальше можно делать а) пытаться из шума градиентным спуском сойтись к чему-то, у чего будет высокий отклик сети (>> 1...), минусы - ноты становятся непрерывными, но казалось бы это самый грамотный подход; б) генетический пооооиск! Плюсы - ноты всё-таки будут 0 и 1. Сходится к вариантам с кучей единиц или одним нулям, т.е. вещам, которых в выборке в помини не было. Видимо, надо добавить в выборку нагенерированных примеров от балды.

## Вставка памяти для поиска повторов
LSTM действительно не воспринимает такую закономерность, как повтор, и это ключевая проблема. Отсюда идея, давайте добавим в сеть внешнюю память, HistoryUser. Есть менее костыльная, но технически сложная в реализации затея следующего вида. Нужно научить модель повтору. Значит, нужно иметь нечто, что будет говорить нейросети, что знаешь, ты уже была вот в том "состоянии", в котором ты сейчас находишься, и правильный ответ тогда был вот такой. Другая интерпретация - память модели есть ещё одна переобученная модель. Например, 1NN. Здесь есть некие параллели с тьюринг-нейронкой, и по этой затеи есть ещё комментарии, но с реализацией сложности; в том числе, скорее всего придётся обучать её эволюционно.

# HistoryUser
В силу то ли кривости моих рук, то ли кривости тех фанатов операционных систем, которые писали TensorBoard, красивые картинки пока там построить не получится. Но ничего!

Итак, очередной вход последовательности проходит через два слоя LSTM, затем линейно преобразуются к вектору размерности 80 и нормируются соофтмаксом. Это - веса для считывания из "истории", то есть предыдущих 80 входов. То, что их фиксированно 80, как бы плохо, но в будущем разумно будет просто оставить те периоды, которые встречаются в мелодиях, и их будет "всех возможных" даже поменьше. При этом нейронки же умеют работать с гигантскими входами, можно бы и просто всю эту историю как вход подавать, но в такой концепции сама суть в том, чтобы генерить для них веса, не глядя туда. После исправления досадных багов, такая "архитектура" лёгким взмахом пары итераций градиентного спуска [радостно сходится к нулевым потерям](https://github.com/FortsAndMills/MusicGeneration/blob/master/Lstm%20HistoryUser%20Test.ipynb). Более того, обучается тупейшим [эволюционным алгоритмом](https://github.com/FortsAndMills/MusicGeneration/blob/master/Lstm%20Evolving%20HistoryUser%20Test.ipynb).

По некоторым причинам, реализация с развёрнутым циклом - да-да, это можно оптимизировать через MultiRNN, но [при переписывании в той же концепции возникли слишком многомерные тензоры](https://github.com/FortsAndMills/MusicGeneration/blob/master/Lstm%20Optimized%20HistoryUser%20Test.ipynb), и оперативки в 16 гигов перестало хватать. Причина в том, что нужно вход X (размер батча x длину последовательности x размерность одного вектора) размножить по времени и перемножать с коэффициентами считывания (для каждого батча и момента времени). Альтернативы пока не придумывается... А переписать в терминах RNN не выходит - на вход такому модулю нужно дополнительно дарить вход с X-ов.

Ну да выход такого "модуля истории" нужно ещё преобразовать, а, возможно, историю вообще не нужно использовать и выдать ответ просто в зависимости от текущего входа (например, в начале, или при "генерации новой части"). Грубо говоря. Поэтому после этого модуля вставим ещё один слой LSTM-ов, который будет получать на вход и выход "истории", и текущий вход. Тут возникает интересный вопрос, что ну а если этот слой будет что-то нетривиальное делать, то правильные ли вектора будут "требоваться" в истории и туда ли градиентный спуск нас поведёт. Поэтому проведён [ряд интересных тестов этой архитектуры](https://github.com/FortsAndMills/MusicGeneration/blob/master/Lstm%20HistoryUser%20Arch.ipynb). Вкратце - работает.

Далее: [эксперимент по засовыванию музыки](https://github.com/FortsAndMills/MusicGeneration/blob/master/Lstm%20HistoryUser%20MusicGenerator.ipynb). Легко переобучается на 35 песенках, однако как генерировать по таким выходам (13 чисел от 0 до 1, независимых, на каждую ноту) не очень понятно. Тупой алгоритм генерации среди не очень хороших результатов выдал и нечто интересное, [HistoryUser(2-layer LSTM) united with inputs (1-layer LSTM) trained on 35 songs (good example).mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/HistoryUser(2-layer%20LSTM)%20united%20with%20inputs%20(1-layer%20LSTM)%20trained%20on%2035%20songs%20(good%20example).mid.mid), но локальные зависимости явно не уловились...

Тут ответ, возможно, такой  - ну а где в такой архитектуре та часть, которая отвечает за "локальные"-то зависимости? Ну то есть супер, есть эта история, появилось улавливание глобальных зависимостей, но надо и локальные-то не потерять (которые, напомню, в статьях улавливались обычными LSTM-ами). Поэтому давайте а) всё-таки введём кросс-энтропию и будем выдавать распределение, чтобы не через одно место сэмплировать б) Вход будем через ещё один LSTM прогонять, и уже выход этого LSTM-а подавать финальному слою, который помимо преобразованного входа увидит и историю, и уже там решит, что со всем этим делать. [Результат получился довольно неожиданный](https://github.com/FortsAndMills/MusicGeneration/blob/master/Lstm%20HistoryUser%20MelodyGenerator.ipynb). Во-первых, за счёт вероятностей на выходе явно сэмплирование проходит адекватнее. А вот в ходе обучения словно LSTM перехватил себе 35 песенок, и модуль истории не уловил, что считывать повторы надо с 32 итераций назад (вместо этого считывает... с 21? Серьёзно? И помогает? Непонятно...). Зато в результатах генерации уже появляется что-то интересное: см. три примера HU (2-layer LSTM) united (1-layer LSTM) with transformed inputs (1-layer LSTM) trained on 35 songs в этой папке.
