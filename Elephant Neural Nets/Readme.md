Краткое саммари: всё ещё вешается :(

# Слоновьи нейросети...

## Идея

* есть задача запоминания выборки (в продолжение [вот этой философии](https://github.com/FortsAndMills/MusicGeneration/tree/master/Discon))
* предлагается попробовать решить её в нейросетевых терминах

### Мотивация:
* формулы, получающиеся в [булевом подходе](https://github.com/FortsAndMills/MusicGeneration/blob/master/Discon/%D0%A1%D0%BE%D0%B1%D1%80%D0%B0%D0%BD%D0%B8%D0%B5%20%D1%81%D0%BE%D1%87%D0%B8%D0%BD%D0%B5%D0%BD%D0%B8%D0%B9...%20%D0%B3%D0%BC%2C%20%D0%B1%D1%83%D0%BB%D0%B5%D0%B2%D1%8B%D1%85.pdf) очень напоминают нейросеть
* любой алгоритм над булевыми входами или булевыми выходами может быть задан булевой формулой, поэтому нейросеть с булевыми входами и выходами можно попытаться строить логическим путём
* нейросетям свойственно дообучение => возможность "выучивать" новые данные
* топологические преобразования могут быть интерпретируемой дискретной заменой градиентной оптимизации
* нейроны в данной концепции превращаются в нечто вроде "понятий", т.е. терминологического языка, которым сеть описывает явления в выборке

## Модель

Пусть количество булевых входов и выходов фиксировано, и имеется выборка. Связей и скрытых слоёв в сети нет; это стартовая инициализация.

Нейроны устроены стандартным образом, проводится суммирование входов и сравнение с нулём (функция Хэвисайда). Возможные альтернативы:  ReLu вместо функции активации; нейроны-диъюнкции и нейроны-конъюнкции; нефиксированный порог (т.е. изменение порога как потенциальное действие при корректировке).

Все веса связей равны 1 или -1 (альтернатива - 0 и 1, но тогда требуется как-то вводить отрицание). Вообще, бинарная дискретизация весов [уже исследовалась](https://arxiv.org/pdf/1602.02830.pdf), и показано, что нейросети не только не теряют в качестве, но и регуляризуются за счёт этого. Дополнительно можно не требовать, что граф сети не является мультиграфом, и тогда фактически появляется возможность образования счётных весов.

### В чём состоит "обучение":

Проводится проход по выборке либо стандартной процедурой, т.е. эпохами, либо запоминается N объектов и затем требуется ещё и K, для чего, возможно, понадобится, пройтись несколько раз и по первым N. Заведомо требуется, что алгоритм должен работать и во втором случае, т.к. это и есть желаемое "дообучение". Возможно полагать K=1 и учить выборку "по одному" объекту, что вычислительно неэффективно, но пока о вычислительной сложности речи не идёт.

Очередной объект подаётся на вход сети, и в случае ошибки на выходном нейроне требуется произвести такие топологические преобразования, чтобы ошибка "исправилась". При этом можно добавлять и удалять связи и нейроны (скрытых слоёв). Пока что без рекуррентности, то есть предполагается защита от циклов: все нейроны уникально нумеруются, и разрешается строить только те связи, где вход имеет меньший номер, чем выход. При этом гарантировать, что сеть не забудет прошлые объекты из выборки, не требуется, пусть проходов будет несколько, основная цель - в итоге сойтись к нулю ошибок на обучении.

## Мечты:
* алгоритм сам оценивает сложность данных и строит столько нейронов и связей, сколько нужно
* в процессе запоминания алгоритму сложно построить вырожденные случаи переобучения (а-ля 1NN), вместо этого ему придётся придумывать правила, отсекать неудачные и оставлять удачные варианты, а т.е. выявлять закономерности
* вообще, в алгоритме нигде явно не требуется явного разделения X и y. Достаточно указать, какие события имели место быть, после чего во всех нейронах, где ожидания алгоритма не совпали с реальностью, проводить корректировку.
* если удастся решить эту задачу + перейти от булевой алгебры к вероятностной, получится алгоритм, генерирующий вероятностную модель по данным.

# Слоник всё ещё вешается

Что я с разной степенью безуспешности пробовал:
- Neural conj-disj: все нейроны превратить в дизъюнкции-конъюнкции и работать в этих нейронах по-разному. Фактически, получается построение общей булевой формулы. Вообще не работало.
- Neural Voting: нейроны-функции голосования, веса 0 и 1, четыре действия (заглушка, причина, гипотеза, рекурсия). Действия выбираются случайно, пробовали разные варианты. Стало лучше после вычисления статистик для связей, после чего появились способы удаления плохих связей.
- Neural conj-disj + crossentropy MC agent: попытка переложить выбор действия на RL
- Neural voting не ломать: пометка, что тут достигнуты относительно самые приличные результаты за счёт экспоненциального захода внутрь сети при помощи статистик уверенности сигнала нейрона.
- Binary Elephant: переход к весам 1 и -1. Действия - создать связь, создать нейрон разрывом существующей связи и удаление. Успеха всё ещё нет
- Binary Elephant Variations: куча разновидностей выбора действия в бинарном слонике 
- Binary Elephant RW: попытка выбирать действие случайным блужданием по сети
- Binary Elephant Path: на каждом шаге выбирается случайный путь, по котором прошла ошибка (чем хуже статистики, тем вероятнее выбрать путь) и там проводится одно изменение. Отсутствует баланс между созданием и удалением нейронов
- Binary Elephant Drones: по сети гуляют дроны удаления связей, добавления нейронов и добавления связей, дроны независимы и сохраняют своё положение между правками. На удивление работает не хуже предыдущих вариантов, но чуда, конечно, не произошло.

Как не думать о слонах?

