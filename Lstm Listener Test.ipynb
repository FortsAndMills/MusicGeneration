{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! out of range!\n",
      "ERROR! out of range!\n",
      "ERROR! out of range!\n"
     ]
    }
   ],
   "source": [
    "from songs import *\n",
    "import copy\n",
    "def addAllTransposedVersions(Songs, song):\n",
    "    while song.transpose(1):\n",
    "        pass\n",
    "\n",
    "    Songs.append(copy.deepcopy(song))\n",
    "    while song.transpose(-1):\n",
    "        Songs.append(copy.deepcopy(song))\n",
    "Songs = []\n",
    "for i in range(1, 35):\n",
    "    addAllTransposedVersions(Songs, Song('basic midi/track (' + str(i) + ').mid'))\n",
    "    \n",
    "def NextBatch():\n",
    "    batch = np.empty((64, 13*128))\n",
    "    y = []\n",
    "    for i in range(64):\n",
    "        song = copy.deepcopy(Songs[np.random.randint(0, len(Songs))])\n",
    "        y.append([np.random.randint(0, 9) * np.random.randint(0, 9)])\n",
    "        song.noise(y[-1][0])\n",
    "        batch[i] = song.notes.flatten()\n",
    "    return batch, (1 - np.array(y) / 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 1.5116\n",
      "Step 200, Minibatch Loss= 0.0512\n",
      "Step 400, Minibatch Loss= 0.0487\n",
      "Step 600, Minibatch Loss= 0.0465\n",
      "Step 800, Minibatch Loss= 0.0420\n",
      "Step 1000, Minibatch Loss= 0.0585\n",
      "Step 1200, Minibatch Loss= 0.0455\n",
      "Step 1400, Minibatch Loss= 0.0599\n",
      "Step 1600, Minibatch Loss= 0.0321\n",
      "Step 1800, Minibatch Loss= 0.0490\n",
      "Step 2000, Minibatch Loss= 0.0503\n",
      "Step 2200, Minibatch Loss= 0.0545\n",
      "Step 2400, Minibatch Loss= 0.0390\n",
      "Step 2600, Minibatch Loss= 0.0485\n",
      "Step 2800, Minibatch Loss= 0.0291\n",
      "Step 3000, Minibatch Loss= 0.0506\n",
      "Step 3200, Minibatch Loss= 0.0238\n",
      "Step 3400, Minibatch Loss= 0.0338\n",
      "Step 3600, Minibatch Loss= 0.0369\n",
      "Step 3800, Minibatch Loss= 0.0301\n",
      "Step 4000, Minibatch Loss= 0.0384\n",
      "Step 4200, Minibatch Loss= 0.0397\n",
      "Step 4400, Minibatch Loss= 0.0471\n",
      "Step 4600, Minibatch Loss= 0.0416\n",
      "Step 4800, Minibatch Loss= 0.0503\n",
      "Step 5000, Minibatch Loss= 0.0459\n",
      "Step 5200, Minibatch Loss= 0.0332\n",
      "Step 5400, Minibatch Loss= 0.0323\n",
      "Step 5600, Minibatch Loss= 0.0320\n",
      "Step 5800, Minibatch Loss= 0.0383\n",
      "Step 6000, Minibatch Loss= 0.0391\n",
      "Step 6200, Minibatch Loss= 0.0505\n",
      "Step 6400, Minibatch Loss= 0.0355\n",
      "Step 6600, Minibatch Loss= 0.0336\n",
      "Step 6800, Minibatch Loss= 0.0374\n",
      "Step 7000, Minibatch Loss= 0.0299\n",
      "Step 7200, Minibatch Loss= 0.0421\n",
      "Step 7400, Minibatch Loss= 0.0641\n",
      "Step 7600, Minibatch Loss= 0.0314\n",
      "Step 7800, Minibatch Loss= 0.0298\n",
      "Step 8000, Minibatch Loss= 0.0306\n",
      "Step 8200, Minibatch Loss= 0.0373\n",
      "Step 8400, Minibatch Loss= 0.0495\n",
      "Step 8600, Minibatch Loss= 0.0543\n",
      "Step 8800, Minibatch Loss= 0.0380\n",
      "Step 9000, Minibatch Loss= 0.0372\n",
      "Step 9200, Minibatch Loss= 0.0368\n",
      "Step 9400, Minibatch Loss= 0.0420\n",
      "Step 9600, Minibatch Loss= 0.0321\n",
      "Step 9800, Minibatch Loss= 0.0254\n",
      "Step 10000, Minibatch Loss= 0.0414\n",
      "Step 10200, Minibatch Loss= 0.0324\n",
      "Step 10400, Minibatch Loss= 0.0373\n",
      "Step 10600, Minibatch Loss= 0.0300\n",
      "Step 10800, Minibatch Loss= 0.0317\n",
      "Step 11000, Minibatch Loss= 0.0446\n",
      "Step 11200, Minibatch Loss= 0.0457\n",
      "Step 11400, Minibatch Loss= 0.0388\n",
      "Step 11600, Minibatch Loss= 0.0355\n",
      "Step 11800, Minibatch Loss= 0.0227\n",
      "Step 12000, Minibatch Loss= 0.0420\n",
      "Step 12200, Minibatch Loss= 0.0471\n",
      "Step 12400, Minibatch Loss= 0.0374\n",
      "Step 12600, Minibatch Loss= 0.0674\n",
      "Step 12800, Minibatch Loss= 0.0349\n",
      "Step 13000, Minibatch Loss= 0.0529\n",
      "Step 13200, Minibatch Loss= 0.0549\n",
      "Step 13400, Minibatch Loss= 0.0461\n",
      "Step 13600, Minibatch Loss= 0.0342\n",
      "Step 13800, Minibatch Loss= 0.0303\n",
      "Step 14000, Minibatch Loss= 0.0334\n",
      "Step 14200, Minibatch Loss= 0.0312\n",
      "Step 14400, Minibatch Loss= 0.0389\n",
      "Step 14600, Minibatch Loss= 0.0452\n",
      "Step 14800, Minibatch Loss= 0.0377\n",
      "Step 15000, Minibatch Loss= 0.0426\n",
      "Step 15200, Minibatch Loss= 0.0437\n",
      "Step 15400, Minibatch Loss= 0.0402\n",
      "Step 15600, Minibatch Loss= 0.0369\n",
      "Step 15800, Minibatch Loss= 0.0429\n",
      "Step 16000, Minibatch Loss= 0.0445\n",
      "Step 16200, Minibatch Loss= 0.0345\n",
      "Step 16400, Minibatch Loss= 0.0344\n",
      "Step 16600, Minibatch Loss= 0.0305\n",
      "Step 16800, Minibatch Loss= 0.0447\n",
      "Step 17000, Minibatch Loss= 0.0543\n",
      "Step 17200, Minibatch Loss= 0.0329\n",
      "Step 17400, Minibatch Loss= 0.0488\n",
      "Step 17600, Minibatch Loss= 0.0233\n",
      "Step 17800, Minibatch Loss= 0.0274\n",
      "Step 18000, Minibatch Loss= 0.0398\n",
      "Step 18200, Minibatch Loss= 0.0368\n",
      "Step 18400, Minibatch Loss= 0.0464\n",
      "Step 18600, Minibatch Loss= 0.0303\n",
      "Step 18800, Minibatch Loss= 0.0394\n",
      "Step 19000, Minibatch Loss= 0.0505\n",
      "Step 19200, Minibatch Loss= 0.0288\n",
      "Step 19400, Minibatch Loss= 0.0339\n",
      "Step 19600, Minibatch Loss= 0.0328\n",
      "Step 19800, Minibatch Loss= 0.0320\n",
      "Step 20000, Minibatch Loss= 0.0332\n",
      "Step 20200, Minibatch Loss= 0.0284\n",
      "Step 20400, Minibatch Loss= 0.0379\n",
      "Step 20600, Minibatch Loss= 0.0363\n",
      "Step 20800, Minibatch Loss= 0.0286\n",
      "Step 21000, Minibatch Loss= 0.0423\n",
      "Step 21200, Minibatch Loss= 0.0371\n",
      "Step 21400, Minibatch Loss= 0.0429\n",
      "Step 21600, Minibatch Loss= 0.0349\n",
      "Step 21800, Minibatch Loss= 0.0421\n",
      "Step 22000, Minibatch Loss= 0.0262\n",
      "Step 22200, Minibatch Loss= 0.0367\n",
      "Step 22400, Minibatch Loss= 0.0493\n",
      "Step 22600, Minibatch Loss= 0.0411\n",
      "Step 22800, Minibatch Loss= 0.0262\n",
      "Step 23000, Minibatch Loss= 0.0291\n",
      "Step 23200, Minibatch Loss= 0.0481\n",
      "Step 23400, Minibatch Loss= 0.0269\n",
      "Step 23600, Minibatch Loss= 0.0313\n",
      "Step 23800, Minibatch Loss= 0.0465\n",
      "Step 24000, Minibatch Loss= 0.0377\n",
      "Step 24200, Minibatch Loss= 0.0332\n",
      "Step 24400, Minibatch Loss= 0.0229\n",
      "Step 24600, Minibatch Loss= 0.0396\n",
      "Step 24800, Minibatch Loss= 0.0240\n",
      "Step 25000, Minibatch Loss= 0.0389\n",
      "Step 25200, Minibatch Loss= 0.0352\n",
      "Step 25400, Minibatch Loss= 0.0326\n",
      "Step 25600, Minibatch Loss= 0.0389\n",
      "Step 25800, Minibatch Loss= 0.0310\n",
      "Step 26000, Minibatch Loss= 0.0289\n",
      "Step 26200, Minibatch Loss= 0.0384\n",
      "Step 26400, Minibatch Loss= 0.0343\n",
      "Step 26600, Minibatch Loss= 0.0329\n",
      "Step 26800, Minibatch Loss= 0.0407\n",
      "Step 27000, Minibatch Loss= 0.0315\n",
      "Step 27200, Minibatch Loss= 0.0361\n",
      "Step 27400, Minibatch Loss= 0.0528\n",
      "Step 27600, Minibatch Loss= 0.0334\n",
      "Step 27800, Minibatch Loss= 0.0347\n",
      "Step 28000, Minibatch Loss= 0.0291\n",
      "Step 28200, Minibatch Loss= 0.0359\n",
      "Step 28400, Minibatch Loss= 0.0319\n",
      "Step 28600, Minibatch Loss= 0.0309\n",
      "Step 28800, Minibatch Loss= 0.0409\n",
      "Step 29000, Minibatch Loss= 0.0316\n",
      "Step 29200, Minibatch Loss= 0.0342\n",
      "Step 29400, Minibatch Loss= 0.0350\n",
      "Step 29600, Minibatch Loss= 0.0328\n",
      "Step 29800, Minibatch Loss= 0.0342\n",
      "Step 30000, Minibatch Loss= 0.0185\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "To classify images using a recurrent neural network, we consider every image\n",
    "row as a sequence of pixels. Because MNIST image shape is 28*28px, we will then\n",
    "handle 28 sequences of 28 steps for every sample.\n",
    "'''\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 5000\n",
    "batch_size = 64\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 13 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 128 # timesteps\n",
    "num_hidden = 100 # hidden layer num of features\n",
    "num_classes = 1 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "#logits = RNN(X, weights, biases)\n",
    "outputs = RNN(X, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "#loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "loss_op = tf.reduce_mean(tf.squared_difference(outputs, Y))\n",
    "#train_op = optimizer.minimize(loss_op)\n",
    "train_op = tf.contrib.layers.optimize_loss(\n",
    "    loss_op, tf.contrib.framework.get_global_step(), optimizer=optimizer,\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "#correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "sess = tf.Session()\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(1, training_steps+1):\n",
    "    batch_x, batch_y = NextBatch()#mnist.train.next_batch(batch_size)\n",
    "    # Reshape data to get 28 seq of 28 elements\n",
    "    batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "    if step % display_step == 0 or step == 1:\n",
    "        # Calculate batch loss and accuracy\n",
    "        L = sess.run([loss_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "        print(\"Step \" + str(step) + \", Minibatch Loss= \" + \"{:.4f}\".format(L[0]))\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x, batch_y = NextBatch()\n",
    "batch_x = batch_x.reshape((batch_size, timesteps, num_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outp = sess.run([outputs], feed_dict={X: batch_x, Y: batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outp = np.array(outp)[0, :, 0]\n",
    "batch_y = batch_y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.97578901,  0.7414425 ,  0.81117874,  0.90079254,  0.9898544 ,\n",
       "        0.85046381,  1.01881909,  0.87322015,  0.81117874,  0.81771642,\n",
       "        0.89996713,  0.86539155,  0.91882735,  0.80229038,  0.93399626], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp[batch_y == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.33812016,  0.71158487,  0.34896761,  0.24188811,  0.45424467,\n",
       "        0.41433007,  0.52510244,  0.48307043,  0.51990777,  0.72696286], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp[batch_y < 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_grad = tf.gradients(outputs, [X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.uniform(0, 1, (1, 128, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.84583712]]\n",
      "[[-4.99764395]]\n",
      "[[-4.08316994]]\n",
      "[[-3.11377263]]\n",
      "[[-2.11158085]]\n",
      "[[-1.10752583]]\n",
      "[[-0.13474917]]\n",
      "[[ 0.77954394]]\n",
      "[[ 1.6190567]]\n",
      "[[ 2.37832475]]\n",
      "[[ 3.05952477]]\n",
      "[[ 3.66892028]]\n",
      "[[ 4.21432972]]\n",
      "[[ 4.70371056]]\n",
      "[[ 5.14451694]]\n",
      "[[ 5.54344749]]\n",
      "[[ 5.906394]]\n",
      "[[ 6.23847723]]\n",
      "[[ 6.54411316]]\n",
      "[[ 6.82707024]]\n",
      "[[ 7.09056568]]\n",
      "[[ 7.33732414]]\n",
      "[[ 7.56964207]]\n",
      "[[ 7.78946686]]\n",
      "[[ 7.99841595]]\n",
      "[[ 8.19787407]]\n",
      "[[ 8.38898277]]\n",
      "[[ 8.57270813]]\n",
      "[[ 8.74986076]]\n",
      "[[ 8.92111778]]\n",
      "[[ 9.08704758]]\n",
      "[[ 9.24812603]]\n",
      "[[ 9.40476704]]\n",
      "[[ 9.55730247]]\n",
      "[[ 9.70602703]]\n",
      "[[ 9.85118961]]\n",
      "[[ 9.99300575]]\n",
      "[[ 10.13166142]]\n",
      "[[ 10.26732063]]\n",
      "[[ 10.40013409]]\n",
      "[[ 10.53022766]]\n",
      "[[ 10.6577282]]\n",
      "[[ 10.78273869]]\n",
      "[[ 10.90535927]]\n",
      "[[ 11.02568531]]\n",
      "[[ 11.14380455]]\n",
      "[[ 11.25979996]]\n",
      "[[ 11.37374115]]\n",
      "[[ 11.48570824]]\n",
      "[[ 11.59576797]]\n",
      "[[ 11.70398235]]\n",
      "[[ 11.81041336]]\n",
      "[[ 11.91512871]]\n",
      "[[ 12.0181694]]\n",
      "[[ 12.11959743]]\n",
      "[[ 12.21946335]]\n",
      "[[ 12.31781578]]\n",
      "[[ 12.41469288]]\n",
      "[[ 12.51014423]]\n",
      "[[ 12.60420799]]\n",
      "[[ 12.69692612]]\n",
      "[[ 12.78833771]]\n",
      "[[ 12.87846947]]\n",
      "[[ 12.9673624]]\n",
      "[[ 13.05505371]]\n",
      "[[ 13.14156914]]\n",
      "[[ 13.22693539]]\n",
      "[[ 13.31118774]]\n",
      "[[ 13.39435005]]\n",
      "[[ 13.47644424]]\n",
      "[[ 13.55750084]]\n",
      "[[ 13.63754463]]\n",
      "[[ 13.71659279]]\n",
      "[[ 13.79468346]]\n",
      "[[ 13.8718195]]\n",
      "[[ 13.94802094]]\n",
      "[[ 14.02331924]]\n",
      "[[ 14.09773636]]\n",
      "[[ 14.17127705]]\n",
      "[[ 14.24397278]]\n",
      "[[ 14.31582832]]\n",
      "[[ 14.3868742]]\n",
      "[[ 14.45710945]]\n",
      "[[ 14.52656841]]\n",
      "[[ 14.59525299]]\n",
      "[[ 14.66318417]]\n",
      "[[ 14.73037148]]\n",
      "[[ 14.79684162]]\n",
      "[[ 14.86258888]]\n",
      "[[ 14.9276371]]\n",
      "[[ 14.99200153]]\n",
      "[[ 15.05569267]]\n",
      "[[ 15.11872005]]\n",
      "[[ 15.18109608]]\n",
      "[[ 15.24283314]]\n",
      "[[ 15.30394077]]\n",
      "[[ 15.36443043]]\n",
      "[[ 15.42431164]]\n",
      "[[ 15.48359585]]\n",
      "[[ 15.5422945]]\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "for i in range(100):\n",
    "    vg, rew = sess.run([var_grad, outputs], feed_dict={X: noise})\n",
    "    noise += np.array(a * vg[0])\n",
    "    print(rew)\n",
    "    a *= 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = (noise > 0.9)[0]\n",
    "test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MySong(test).save_file(\"LSTM listener output maximized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## а ещё йауметьвгенетику"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = np.random.choice([0, 1], (1000, 128, 13), p=[0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.34529352] [ 0.3968932]\n",
      "[-0.01950306] [ 0.46789747]\n",
      "[ 0.2118029] [ 0.58066291]\n",
      "[ 0.40004498] [ 0.69723481]\n",
      "[ 0.55638939] [ 0.78155404]\n",
      "[ 0.6818009] [ 0.86992091]\n",
      "[ 0.77621418] [ 0.94908482]\n",
      "[ 0.8563686] [ 0.993213]\n",
      "[ 0.92054552] [ 1.01440358]\n",
      "[ 0.97190648] [ 1.06331015]\n",
      "[ 1.01419592] [ 1.07445812]\n",
      "[ 1.04561257] [ 1.10207033]\n",
      "[ 1.07110286] [ 1.11727452]\n",
      "[ 1.09249878] [ 1.12866402]\n",
      "[ 1.11143422] [ 1.13712382]\n",
      "[ 1.1242702] [ 1.14243388]\n",
      "[ 1.13467026] [ 1.15202308]\n",
      "[ 1.14299679] [ 1.15806055]\n",
      "[ 1.14917779] [ 1.1593225]\n",
      "[ 1.15447164] [ 1.16132617]\n",
      "[ 1.15858865] [ 1.16584468]\n",
      "[ 1.16175127] [ 1.1665318]\n",
      "[ 1.16426682] [ 1.16876602]\n",
      "[ 1.16628718] [ 1.16898417]\n",
      "[ 1.16784787] [ 1.1706264]\n"
     ]
    }
   ],
   "source": [
    "for gen in range(25):\n",
    "    rewards = sess.run([outputs], feed_dict={X: population})[0]\n",
    "\n",
    "    i = np.argsort(rewards.flatten())[-250:]\n",
    "    print(rewards[i[0]], rewards[i[-1]])\n",
    "    population = population[i]\n",
    "\n",
    "    mothers = np.random.randint(0, 250, 1000)\n",
    "    fathers = np.random.randint(0, 250, 1000)\n",
    "    children = (population[mothers] + population[fathers]) / 2\n",
    "    children[children == 0.5] = np.random.choice([0, 1], ((children == 0.5).sum()), p=[0.5, 0.5])\n",
    "\n",
    "    population = children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "MySong(population[-1]).play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
