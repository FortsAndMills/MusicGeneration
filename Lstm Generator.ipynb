{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! out of range!\n",
      "ERROR! out of range!\n",
      "ERROR! out of range!\n"
     ]
    }
   ],
   "source": [
    "from songs import *\n",
    "import copy\n",
    "def addAllTransposedVersions(Songs, song):\n",
    "    while song.transpose(1):\n",
    "        pass\n",
    "\n",
    "    Songs.append(copy.deepcopy(song))\n",
    "    while song.transpose(-1):\n",
    "        Songs.append(copy.deepcopy(song))\n",
    "Songs = []\n",
    "for i in range(1, 35):\n",
    "    addAllTransposedVersions(Songs, Song('basic midi/track (' + str(i) + ').mid'))\n",
    "    \n",
    "def NextBatch():\n",
    "    batch = np.empty((64, 13*128))\n",
    "    y = []\n",
    "    for i in range(64):\n",
    "        song = copy.deepcopy(Songs[np.random.randint(0, len(Songs))])\n",
    "        y.append([np.random.randint(0, 9) * np.random.randint(0, 9)])\n",
    "        song.noise(y[-1][0])\n",
    "        batch[i] = song.notes.flatten()\n",
    "    return batch, (1 - np.array(y) / 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_songs = np.array([song.notes for song in Songs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 20000\n",
    "batch_size = all_songs.shape[0]\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 13\n",
    "timesteps = 128 # timesteps\n",
    "num_hidden = 100 # hidden layer num of features\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "\n",
    "# Define weights\n",
    "w = tf.Variable(tf.random_normal([num_hidden, num_input]))\n",
    "b = tf.Variable(tf.random_normal([num_input]))\n",
    "\n",
    "class lstm:\n",
    "    def __init__(self, name, x, num_of_neurons):\n",
    "        with tf.variable_scope(name):\n",
    "            self.lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "            self.hidden_state = tf.zeros([batch_size, num_of_neurons])\n",
    "            self.current_state = tf.zeros([batch_size, num_of_neurons])\n",
    "            state = self.hidden_state, self.current_state\n",
    "            self.outputs = []\n",
    "            for inp in x:\n",
    "                output, state = self.lstm_cell(inp, state)\n",
    "                self.outputs.append(output)\n",
    "\n",
    "_X = tf.unstack(X, timesteps, 1)\n",
    "first_layer = lstm(\"fl\", _X, num_hidden)\n",
    "second_layer = lstm(\"sl\", first_layer.outputs, num_hidden)\n",
    "\n",
    "logits = tf.convert_to_tensor([tf.sigmoid(tf.matmul(second_layer.outputs[i], w) + b) for i in range(timesteps)])\n",
    "\n",
    "# Define loss and optimizer\n",
    "#loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=X))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "loss_op = -tf.reduce_mean(0.97 * tf.multiply(tf.convert_to_tensor(_X), tf.log(logits)) +\n",
    "                         0.03 * tf.multiply(1. - tf.convert_to_tensor(_X), tf.log(1. - logits)))\n",
    "#train_op = optimizer.minimize(loss_op)\n",
    "train_op = tf.contrib.layers.optimize_loss(\n",
    "    loss_op, tf.contrib.framework.get_global_step(), optimizer=optimizer,\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "#correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Minibatch Loss= 0.0458\n",
      "Step 200, Minibatch Loss= 0.0453\n",
      "Step 400, Minibatch Loss= 0.0449\n",
      "Step 600, Minibatch Loss= 0.0445\n",
      "Step 800, Minibatch Loss= 0.0442\n",
      "Step 1000, Minibatch Loss= 0.0438\n",
      "Step 1200, Minibatch Loss= 0.0435\n",
      "Step 1400, Minibatch Loss= 0.0432\n",
      "Step 1600, Minibatch Loss= 0.0429\n",
      "Step 1800, Minibatch Loss= 0.0427\n",
      "Step 2000, Minibatch Loss= 0.0424\n",
      "Step 2200, Minibatch Loss= 0.0422\n",
      "Step 2400, Minibatch Loss= 0.0420\n",
      "Step 2600, Minibatch Loss= 0.0418\n",
      "Step 2800, Minibatch Loss= 0.0416\n",
      "Step 3000, Minibatch Loss= 0.0415\n",
      "Step 3200, Minibatch Loss= 0.0413\n",
      "Step 3400, Minibatch Loss= 0.0412\n",
      "Step 3600, Minibatch Loss= 0.0410\n",
      "Step 3800, Minibatch Loss= 0.0409\n",
      "Step 4000, Minibatch Loss= 0.0408\n",
      "Step 4200, Minibatch Loss= 0.0407\n",
      "Step 4400, Minibatch Loss= 0.0405\n",
      "Step 4600, Minibatch Loss= 0.0404\n",
      "Step 4800, Minibatch Loss= 0.0403\n",
      "Step 5000, Minibatch Loss= 0.0403\n",
      "Step 5200, Minibatch Loss= 0.0402\n",
      "Step 5400, Minibatch Loss= 0.0401\n",
      "Step 5600, Minibatch Loss= 0.0400\n",
      "Step 5800, Minibatch Loss= 0.0400\n",
      "Step 6000, Minibatch Loss= 0.0399\n",
      "Step 6200, Minibatch Loss= 0.0398\n",
      "Step 6400, Minibatch Loss= 0.0398\n",
      "Step 6600, Minibatch Loss= 0.0397\n",
      "Step 6800, Minibatch Loss= 0.0397\n",
      "Step 7000, Minibatch Loss= 0.0396\n",
      "Step 7200, Minibatch Loss= 0.0396\n",
      "Step 7400, Minibatch Loss= 0.0395\n",
      "Step 7600, Minibatch Loss= 0.0395\n",
      "Step 7800, Minibatch Loss= 0.0394\n",
      "Step 8000, Minibatch Loss= 0.0394\n",
      "Step 8200, Minibatch Loss= 0.0394\n",
      "Step 8400, Minibatch Loss= 0.0393\n",
      "Step 8600, Minibatch Loss= 0.0393\n",
      "Step 8800, Minibatch Loss= 0.0392\n",
      "Step 9000, Minibatch Loss= 0.0392\n",
      "Step 9200, Minibatch Loss= 0.0392\n",
      "Step 9400, Minibatch Loss= 0.0392\n",
      "Step 9600, Minibatch Loss= 0.0391\n",
      "Step 9800, Minibatch Loss= 0.0391\n",
      "Step 10000, Minibatch Loss= 0.0391\n",
      "Step 10200, Minibatch Loss= 0.0391\n",
      "Step 10400, Minibatch Loss= 0.0390\n",
      "Step 10600, Minibatch Loss= 0.0390\n",
      "Step 10800, Minibatch Loss= 0.0390\n",
      "Step 11000, Minibatch Loss= 0.0390\n",
      "Step 11200, Minibatch Loss= 0.0389\n",
      "Step 11400, Minibatch Loss= 0.0389\n",
      "Step 11600, Minibatch Loss= 0.0389\n",
      "Step 11800, Minibatch Loss= 0.0389\n",
      "Step 12000, Minibatch Loss= 0.0389\n",
      "Step 12200, Minibatch Loss= 0.0389\n",
      "Step 12400, Minibatch Loss= 0.0388\n",
      "Step 12600, Minibatch Loss= 0.0388\n",
      "Step 12800, Minibatch Loss= 0.0388\n",
      "Step 13000, Minibatch Loss= 0.0388\n",
      "Step 13200, Minibatch Loss= 0.0388\n",
      "Step 13400, Minibatch Loss= 0.0388\n",
      "Step 13600, Minibatch Loss= 0.0388\n",
      "Step 13800, Minibatch Loss= 0.0387\n",
      "Step 14000, Minibatch Loss= 0.0387\n",
      "Step 14200, Minibatch Loss= 0.0387\n",
      "Step 14400, Minibatch Loss= 0.0387\n",
      "Step 14600, Minibatch Loss= 0.0387\n",
      "Step 14800, Minibatch Loss= 0.0387\n",
      "Step 15000, Minibatch Loss= 0.0387\n",
      "Step 15200, Minibatch Loss= 0.0387\n",
      "Step 15400, Minibatch Loss= 0.0386\n",
      "Step 15600, Minibatch Loss= 0.0386\n",
      "Step 15800, Minibatch Loss= 0.0386\n",
      "Step 16000, Minibatch Loss= 0.0386\n",
      "Step 16200, Minibatch Loss= 0.0386\n",
      "Step 16400, Minibatch Loss= 0.0386\n",
      "Step 16600, Minibatch Loss= 0.0386\n",
      "Step 16800, Minibatch Loss= 0.0386\n",
      "Step 17000, Minibatch Loss= 0.0386\n",
      "Step 17200, Minibatch Loss= 0.0386\n",
      "Step 17400, Minibatch Loss= 0.0386\n",
      "Step 17600, Minibatch Loss= 0.0386\n",
      "Step 17800, Minibatch Loss= 0.0385\n",
      "Step 18000, Minibatch Loss= 0.0385\n",
      "Step 18200, Minibatch Loss= 0.0385\n",
      "Step 18400, Minibatch Loss= 0.0385\n",
      "Step 18600, Minibatch Loss= 0.0385\n",
      "Step 18800, Minibatch Loss= 0.0385\n",
      "Step 19000, Minibatch Loss= 0.0385\n",
      "Step 19200, Minibatch Loss= 0.0385\n",
      "Step 19400, Minibatch Loss= 0.0385\n",
      "Step 19600, Minibatch Loss= 0.0385\n",
      "Step 19800, Minibatch Loss= 0.0385\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "sess = tf.Session()\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(0, training_steps):\n",
    "    batch_x = all_songs#mnist.train.next_batch(batch_size)\n",
    "    batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "    \n",
    "    # Run optimization op (backprop)\n",
    "    sess.run(train_op, feed_dict={X: batch_x})\n",
    "    if step % display_step == 0:\n",
    "        # Calculate batch loss and accuracy\n",
    "        L = sess.run(loss_op, feed_dict={X: batch_x})\n",
    "        print(\"Step \" + str(step) + \", Minibatch Loss= \" + \"{:.4f}\".format(L))\n",
    "\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_songs = np.array([song.notes for song in Songs])\n",
    "batch_x = all_songs\n",
    "batch_x = batch_x.reshape((69, timesteps, num_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outp = np.array(sess.run(logits, feed_dict={X: batch_x}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_song = outp[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 128 but corresponding boolean dimension is 32\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.35305798,  0.40715349,  0.43449512,  0.51579624,  0.4916901 ,\n",
       "        0.57061625,  0.56180316,  0.54283768,  0.56193489,  0.53043222,\n",
       "        0.55983573,  0.52543014,  0.53507745,  0.51688242], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_song[all_songs[0][:32] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: VisibleDeprecationWarning: boolean index did not match indexed array along dimension 0; dimension is 128 but corresponding boolean dimension is 32\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.35305798,  0.40715349,  0.43449512,  0.51579624,  0.4916901 ,\n",
       "        0.57061625,  0.56180316,  0.54283768,  0.56193489,  0.53043222,\n",
       "        0.55983573,  0.52543014,  0.53507745], dtype=float32)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_song[all_songs[0][32:64] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема: LSTM ни фига не запоминает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_songs = [MySong() for i in range(69)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(128):\n",
    "    outp = np.array(sess.run(logits, feed_dict={X: batch_x}))\n",
    "    for s in range(69):\n",
    "        next_one = outp[i, s, :]\n",
    "            \n",
    "        next_one[next_one < 0.65] = 0\n",
    "        next_one[next_one != 0] = 1\n",
    "        \n",
    "        if i >= 8:\n",
    "            batch_x[s, i] = next_one\n",
    "            new_songs[s].add(next_one)\n",
    "        else:\n",
    "            new_songs[s].add(batch_x[s, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_songs[0].play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
