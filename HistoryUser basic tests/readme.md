# HistoryUser
В силу то ли кривости моих рук, то ли кривости тех фанатов операционных систем, которые писали TensorBoard, красивые картинки пока там построить не получится. Но ничего!

Итак, очередной вход последовательности проходит через два слоя LSTM, затем линейно преобразуются к вектору размерности 80 и нормируются соофтмаксом. Это - веса для считывания из "истории", то есть предыдущих 80 входов. То, что их фиксированно 80, как бы плохо, но в будущем разумно будет просто оставить те периоды, которые встречаются в мелодиях, и их будет "всех возможных" даже поменьше. При этом нейронки же умеют работать с гигантскими входами, можно бы и просто всю эту историю как вход подавать, но в такой концепции сама суть в том, чтобы генерить для них веса, не глядя туда. После исправления досадных багов, такая "архитектура" лёгким взмахом пары итераций градиентного спуска [радостно сходится к нулевым потерям](https://github.com/FortsAndMills/MusicGeneration/blob/master/Lstm%20HistoryUser%20Test.ipynb). Более того, обучается тупейшим [эволюционным алгоритмом](https://github.com/FortsAndMills/MusicGeneration/blob/master/Lstm%20Evolving%20HistoryUser%20Test.ipynb).

По некоторым причинам, реализация с развёрнутым циклом - да-да, это можно оптимизировать через MultiRNN, но [при переписывании в той же концепции возникли слишком многомерные тензоры](https://github.com/FortsAndMills/MusicGeneration/blob/master/Lstm%20Optimized%20HistoryUser%20Test.ipynb), и оперативки в 16 гигов перестало хватать. Причина в том, что нужно вход X (размер батча x длину последовательности x размерность одного вектора) размножить по времени и перемножать с коэффициентами считывания (для каждого батча и момента времени). Альтернативы пока не придумывается... А переписать в терминах RNN не выходит - на вход такому модулю нужно дополнительно дарить вход с X-ов.

Ну да выход такого "модуля истории" нужно ещё преобразовать, а, возможно, историю вообще не нужно использовать и выдать ответ просто в зависимости от текущего входа (например, в начале, или при "генерации новой части"). Грубо говоря. Поэтому после этого модуля вставим ещё один слой LSTM-ов, который будет получать на вход и выход "истории", и текущий вход. Тут возникает интересный вопрос, что ну а если этот слой будет что-то нетривиальное делать, то правильные ли вектора будут "требоваться" в истории и туда ли градиентный спуск нас поведёт. Поэтому проведён [ряд интересных тестов этой архитектуры](https://github.com/FortsAndMills/MusicGeneration/blob/master/Lstm%20HistoryUser%20Arch.ipynb). Вкратце - работает.

Далее: [эксперимент по засовыванию музыки](https://github.com/FortsAndMills/MusicGeneration/blob/master/Lstm%20HistoryUser%20MusicGenerator.ipynb). Легко переобучается на 35 песенках, однако как генерировать по таким выходам (13 чисел от 0 до 1, независимых, на каждую ноту) не очень понятно. Тупой алгоритм генерации среди не очень хороших результатов выдал и нечто интересное, [HistoryUser(2-layer LSTM) united with inputs (1-layer LSTM) trained on 35 songs (good example).mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/HistoryUser(2-layer%20LSTM)%20united%20with%20inputs%20(1-layer%20LSTM)%20trained%20on%2035%20songs%20(good%20example).mid.mid), но локальные зависимости явно не уловились...

Тут ответ, возможно, такой  - ну а где в такой архитектуре та часть, которая отвечает за "локальные"-то зависимости? Ну то есть супер, есть эта история, появилось улавливание глобальных зависимостей, но надо и локальные-то не потерять (которые, напомню, в статьях улавливались обычными LSTM-ами). Поэтому давайте а) всё-таки введём кросс-энтропию и будем выдавать распределение, чтобы не через одно место сэмплировать б) Вход будем через ещё один LSTM прогонять, и уже выход этого LSTM-а подавать финальному слою, который помимо преобразованного входа увидит и историю, и уже там решит, что со всем этим делать. [Результат получился довольно неожиданный](https://github.com/FortsAndMills/MusicGeneration/blob/master/Lstm%20HistoryUser%20MelodyGenerator.ipynb). Во-первых, за счёт вероятностей на выходе явно сэмплирование проходит адекватнее. А вот в ходе обучения словно LSTM перехватил себе 35 песенок, и модуль истории не уловил, что считывать повторы надо с 32 итераций назад (вместо этого считывает... с 21? Серьёзно? И помогает? Непонятно...). Зато в результатах генерации уже появляется что-то интересное: см. три примера HU (2-layer LSTM) united (1-layer LSTM) with transformed inputs (1-layer LSTM) trained on 35 songs в этой папке.
