{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input data is 128+64 random binary vectors of dimension 13\n",
    "# the output data has same dimensions; final 128 of vectors must be same as first 128 of input\n",
    "def generate():\n",
    "    X = np.random.randint(0, 2, (128, 128+64, 13))\n",
    "    X[:, 128:, :] = 0\n",
    "    Y = np.zeros((128, 128+64, 13))\n",
    "    Y[:, 64:, :] = X[:, :128, :]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the input data is 128+64 random binary vectors of dimension 13\n",
    "# the output data has same dimensions; final 128 of vectors must be same as first 128 of input\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 13       # dimension of one input at moment\n",
    "timesteps = 128+64   # timesteps in one sequence\n",
    "num_hidden = 70      # num of lstms in first hidden layer\n",
    "num_read = 50        # num of lstms in second hidden layer\n",
    "history_size = 80    # how much previous moments of input is stored\n",
    "\n",
    "# tf graph input\n",
    "X = tf.placeholder(\"float\", [batch_size, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [batch_size, timesteps, num_input])\n",
    "\n",
    "# loss will be calculated in the loop as sum of losses on each iteration\n",
    "loss_op = tf.constant(0.0)\n",
    "\n",
    "# defining first hidden layer\n",
    "with tf.variable_scope('first_lstm'):\n",
    "    first_lstm = tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "    first_lstm_state = tf.zeros([batch_size, num_hidden]), tf.zeros([batch_size, num_hidden])\n",
    "\n",
    "# defining second hidden layer\n",
    "with tf.variable_scope(\"read_lstm\"):\n",
    "    read_lstm = tf.contrib.rnn.BasicLSTMCell(num_read, forget_bias=1.0)\n",
    "    read_lstm_state = tf.zeros([batch_size, num_read]), tf.zeros([batch_size, num_read])\n",
    "\n",
    "# linear transformation of second lstm cell outputs to history size dimension vector\n",
    "read_w = tf.Variable(tf.random_normal([num_read, history_size]))\n",
    "read_b = tf.Variable(tf.random_normal([history_size]))\n",
    "\n",
    "# history is an array of previous (in time) history_size inputs\n",
    "history = tf.zeros([batch_size, num_input, history_size])\n",
    "\n",
    "# reshaping data to (timesteps, batch_size, num_input)\n",
    "_X = tf.unstack(X, timesteps, 1)\n",
    "_Y = tf.unstack(Y, timesteps, 1)\n",
    "\n",
    "# time loop\n",
    "for t, inp, truth in zip(np.arange(timesteps), _X, _Y):\n",
    "    # moving data through lstm layers\n",
    "    with tf.variable_scope('first_lstm'):\n",
    "        first_lstm_output, first_lstm_state = first_lstm(inp, first_lstm_state)\n",
    "    with tf.variable_scope(\"read_lstm\"):\n",
    "        read_lstm_output, read_lstm_state = read_lstm(first_lstm_output, read_lstm_state)\n",
    "\n",
    "    # non-linear transformation to vector of history_size length\n",
    "    read = tf.matmul(read_lstm_output, read_w) + read_b\n",
    "    # transformation to \"probabilities\"-kind weights\n",
    "    read_proba = tf.nn.softmax(read, 1)    \n",
    "    # taking selected data from memory\n",
    "    memory_retrieve = tf.matmul(history, tf.expand_dims(read_proba, 2))\n",
    "\n",
    "    # calculate loss as cross_entropy\n",
    "    if t >= 64:\n",
    "        loss_op += tf.reduce_mean(tf.squared_difference(memory_retrieve, tf.expand_dims(truth, 2)))\n",
    "        #tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=memory_retrieve, labels=tf.expand_dims(truth, 2)))\n",
    "\n",
    "    # rolling history\n",
    "    history = tf.concat([history[:, :, 1:], tf.expand_dims(inp, 2)], axis=2)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "config = tf.ConfigProto(log_device_placement=True)  # to output is variable on gpu or cpu\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4  # BlasGEMM launch failed error fix (seems like not enopugh memory on gpu)\n",
    "sess = tf.Session(config = config)\n",
    "# Run the initializer\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "POPULATION_SIZE = 200\n",
    "TEST_OBJECTS_FOR_FITNESS = 200\n",
    "NOISE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dna:\n",
    "    def __init__(self, weights_values):\n",
    "        self.w = weights_values\n",
    "        \n",
    "    def fitness(self, batch_x, batch_y):\n",
    "        return sess.run(loss_op, feed_dict=\n",
    "                               {X: batch_x, \n",
    "                                Y: batch_y,\n",
    "                                first_lstm.variables[0]: self.w[0],\n",
    "                                first_lstm.variables[1]: self.w[1],\n",
    "                                read_lstm.variables[0]: self.w[2],\n",
    "                                read_lstm.variables[1]: self.w[3],\n",
    "                                read_w: self.w[4],\n",
    "                                read_b: self.w[5]})\n",
    "        \n",
    "    def mutate(self):\n",
    "        new_w = []\n",
    "        for caps in self.w:\n",
    "            new_w.append(caps + NOISE * norm.rvs(size=caps.shape))\n",
    "        return dna(new_w)\n",
    "    \n",
    "    def __mul__(self, a):\n",
    "        new_w = []\n",
    "        for caps in self.w:\n",
    "            new_w.append(caps * a)\n",
    "        return dna(new_w)\n",
    "    \n",
    "    def __add__(self, other_dna):\n",
    "        new_w = []\n",
    "        for caps1, caps2 in zip(self.w, other_dna.w):\n",
    "            new_w.append(caps1 + caps2)\n",
    "        return dna(new_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = dna(sess.run([*first_lstm.variables, *read_lstm.variables, read_w, read_b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen  0  started, res =  36.7912\n",
      "Gen  1  started, res =  36.5913\n",
      "Gen  2  started, res =  36.6216\n",
      "Gen  3  started, res =  36.3381\n",
      "Gen  4  started, res =  36.2395\n",
      "Gen  5  started, res =  36.0752\n",
      "Gen  6  started, res =  35.9586\n",
      "Gen  7  started, res =  36.0479\n",
      "Gen  8  started, res =  35.8683\n",
      "Gen  9  started, res =  35.5271\n",
      "Gen  10  started, res =  35.4795\n",
      "Gen  11  started, res =  35.3467\n",
      "Gen  12  started, res =  34.8382\n",
      "Gen  13  started, res =  34.799\n",
      "Gen  14  started, res =  34.4498\n",
      "Gen  15  started, res =  34.227\n",
      "Gen  16  started, res =  33.823\n",
      "Gen  17  started, res =  33.7685\n",
      "Gen  18  started, res =  33.4723\n",
      "Gen  19  started, res =  33.1612\n",
      "Gen  20  started, res =  32.2172\n",
      "Gen  21  started, res =  32.1463\n",
      "Gen  22  started, res =  30.6981\n",
      "Gen  23  started, res =  29.6556\n",
      "Gen  24  started, res =  29.6219\n",
      "Gen  25  started, res =  26.946\n",
      "Gen  26  started, res =  25.1035\n",
      "Gen  27  started, res =  22.0492\n",
      "Gen  28  started, res =  22.7234\n",
      "Gen  29  started, res =  21.5068\n",
      "Gen  30  started, res =  20.8011\n",
      "Gen  31  started, res =  19.1408\n",
      "Gen  32  started, res =  18.311\n",
      "Gen  33  started, res =  17.3016\n",
      "Gen  34  started, res =  14.761\n",
      "Gen  35  started, res =  12.8177\n",
      "Gen  36  started, res =  9.72806\n",
      "Gen  37  started, res =  7.52694\n",
      "Gen  38  started, res =  5.91139\n",
      "Gen  39  started, res =  5.33328\n",
      "Gen  40  started, res =  3.81136\n",
      "Gen  41  started, res =  2.60487\n",
      "Gen  42  started, res =  1.36148\n",
      "Gen  43  started, res =  0.630268\n",
      "Gen  44  started, res =  0.356857\n",
      "Gen  45  started, res =  0.329193\n",
      "Gen  46  started, res =  0.151366\n",
      "Gen  47  started, res =  0.143253\n",
      "Gen  48  started, res =  0.0956896\n",
      "Gen  49  started, res =  0.074837\n",
      "Gen  50  started, res =  0.0446695\n",
      "Gen  51  started, res =  0.0288128\n",
      "Gen  52  started, res =  0.0232872\n",
      "Gen  53  started, res =  0.0172475\n",
      "Gen  54  started, res =  0.0146286\n",
      "Gen  55  started, res =  0.0108465\n",
      "Gen  56  started, res =  0.00945925\n",
      "Gen  57  started, res =  0.00631498\n",
      "Gen  58  started, res =  0.00534539\n",
      "Gen  59  started, res =  0.00337316\n",
      "Gen  60  started, res =  0.00267894\n",
      "Gen  61  started, res =  0.00193988\n",
      "Gen  62  started, res =  0.00163607\n",
      "Gen  63  started, res =  0.0016734\n",
      "Gen  64  started, res =  0.00204362\n",
      "Gen  65  started, res =  0.00149457\n",
      "Gen  66  started, res =  0.00197529\n",
      "Gen  67  started, res =  0.00141017\n",
      "Gen  68  started, res =  0.00136603\n",
      "Gen  69  started, res =  0.0014067\n",
      "Final res =  0.00151344\n"
     ]
    }
   ],
   "source": [
    "for i in range(70):\n",
    "    # sample new 200 examples from data\n",
    "    batch_x, batch_y = generate()\n",
    "    # check how good we are\n",
    "    print(\"Gen \", i, \" started, res = \", center.fitness(batch_x, batch_y))\n",
    "    \n",
    "    population = [center.mutate() for i in range(POPULATION_SIZE)]\n",
    "    losses = np.array([population[i].fitness(batch_x, batch_y) for i in range(POPULATION_SIZE)])\n",
    "    fitness = losses.max() - losses\n",
    "    \n",
    "    center = np.array(population).dot(fitness) * (1 / fitness.sum())\n",
    "    \n",
    "\n",
    "batch_x, batch_y = generate()\n",
    "print(\"Final res = \", center.fitness(batch_x, batch_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0015134425"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "center.fitness(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
