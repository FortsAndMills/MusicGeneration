Ниже представлено описание основных результатов эксперимента в формате "хачу результат".

## 03/12 - Первая рабочая модель на реальных данных.
[LSTM simple model.ipynb](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/LSTM%20real%20data%20simple%20model.ipynb)

* Помимо текущего вектора (размер одного вектора - 88) на вход подавались вектора 3, 7, 15, 31, 47, 63, 95 и 127 моментов времени назад. Это, наверное, самая банальная модель памяти, какую только можно придумать.
* Датасет (~100 песен) разбивался на фрагменты по 256 моментов времени, старты с каждых двух тактов (такт - 16 моментов времени).
* Обработка однослойным LSTM-ом (300 нейронов).
* На выходе 88 вероятностей, с которыми нужно засэмпировать очередную ноту.

В принципе, это уже что-то. [300 LSTM CM_dataset songs by fragments 8-history 12000 steps lr=0.01.mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/300%20LSTM%20CM_dataset%20songs%20by%20fragments%208-history%2012000%20steps%20lr%3D0.01.mid)

Однако, модель сэмплирования явно неадекватна с теоретической точки зрения. Допустим, на следующем шаге мы можем уйти, скажем, в до мажор и сыграть до-ми-соль, или в фа мажор, фа-ля-до. Идэал'ный генератор Распределения Музыки в текущем формате вывода засэмплирует до, и скажет, что ноты ми, соль, фа и ля должна засэмплироваться с вероятностями 50% каждая. Естественно, что с высокой вероятностью по такому выходу генерируется какая-то ерунда.

Решение: сводить к сэмплированиям из дискретных распределений, т.е. брать на финальном слое софтмакс. Однако, как тогда сэмплировать больше одной ноты?

### 06/12 (failed) - Первая модель полифонического сэмплирования
(LSTM real data polyphonic model.ipynb) Проба старой идеи: на выходе сети выдаём вероятность p, из которого при генерации будет засэмплирована только одна нота. Затем подаём на вход LSTM-у тот же самый вектор, и вероятность p, получаем p2, из которого при генерации сэмплируем вторую ноту. Снова подаём на вход LSTM-у тот же вход, и p + (1 - p)p2... И так, скажем, 7 раз.

"Время" от такого безобразия с точки зрения сети замедляется ещё в 7 раз, что уже само по себе плохо. С этим можно было бы бороться, но идея не выгорела - обучалось плохо (даже пришлось использовать "сжатое" представление до 12-мерных векторов), и генерировалась ерунда.

### 09/12 (failed) - Проба LatencyLSTM
(LatencyLSTM test.ipynb) Основная идея - в рекуррентных связях иметь не один элемент задержки, как это принято, а много... Например, 3, 7, 15, 31... Ну вы поняли. Однако, то ли я в реализации косячил, то ли ещё что, но заменить таким образом модель памяти (и таким образом, сильно упростить вычислительную сложность модели за счёт уменьшения размра входа с 88x9 до 88) не получалось - при обучении "повторы" не обнаруживались.

Что объясняется таким странным фактом, что нужно "смотреть" на 3, 7 и т.д. моментов времени назад, когда период на самом деле 4, 8 и т.д. То есть в нейрон приходит его выход с момента времени, когда на входе был ныне искомый ответ, что означает, что нейрон должен был бы быть занят в тот момент тождественным преобразованием. Что означает, что он не может делать что-то ещё... Проблема распространяется и на LSTM, эксперименты с неединичными периодами с памятью закончились также. Вообще, нормальное решение этой проблемы может лежать в коррекции LSTM-формул, но...

## 11/12 - "Одновременное" полифоническое сэмплирование
[LSTM real data polyphonic one-time model.ipynb](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/LSTM%20real%20data%20polyphonic%20one-time%20model.ipynb)

Нашлось простое и забавное решение (прим. - не сильно снижающее неадекватность, но снижающее) проблемы с полифоническим сэмплированием. А давайте просто сгенерируем на выходе нейросети 7 дискретных 88-мерных распределений (точнее - 89-мерных, ещё один вариант это "пауза", не генерить ноту в этом "голосе")!

Решение рабочее:
[300 LSTM CM_dataset songs by fragments 8-history 7-voices 2500 steps lr=0.01.mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/300%20LSTM%20CM_dataset%20songs%20by%20fragments%208-history%207-voices%202500%20steps%20lr%3D0.01.mid)
[300 LSTM CM_dataset songs by fragments 8-history 7-voices 2500 steps lr=0.01 ex.2.mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/300%20LSTM%20CM_dataset%20songs%20by%20fragments%208-history%2012000%20steps%20lr%3D0.01.mid)

## 13/12 - "Метадообучение", а точнее пародия на него
При генерации нейросеть ДООБУЧАЕТСЯ на том, что она сгенерировала.

Это было бы метаобучением, очень вписывающимся в поставленную задачу, если бы веса сети (на момента старта генерации) подбирались с учётом дообучения по ходу последовательности. Несложно показать, что для этого нужен гессиан. Поэтому проба в костыльном варианте.

Результаты... прикольные.
[300 LSTM CM_dataset songs by fragments 8-history 7-voices 2500 steps lr=0.01 meta_learning=0.0001 1 iter from common.mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/300%20LSTM%20CM_dataset%20songs%20by%20fragments%208-history%207-voices%202500%20steps%20lr%3D0.01%20meta_learning%3D0.0001%201%20iter%20from%20common.mid)

Однако, усиливается риск слиться в генерацию тривиальных последовательностей. Потенциально, метадообучение должно бороться с проблемой "смены песни каждые две секунды". Как и модель памяти...

## 14/12 - Upgraded Version
* Два слоя LSTM по 100 нейронов вместо одного из 300
* На вход дополнительно подаётся время. Моё самодовольство от гениальности этого хода было уничтожено радостным обнаружением, что так давно уже все в генерации музыки делают.

Результаты:
[100-100 LSTM CM_dataset songs by fragments 8-history 6-times 7-voices (one-time) lr=0.01 40000 epochs.mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/100-100%20LSTM%20CM_dataset%20songs%20by%20fragments%208-history%206-times%207-voices%20(one-time)%20lr%3D0.01%2040000%20epochs.mid)

### 15/12 - Вариации псевдометадообучения...
C "метадообучением" Upgraded-модель выглядит... кхм, звучит так:
[100-100 LSTM CM_dataset songs by fragments 8-history 7-voices 6-times 40000 steps lr=0.01 meta_learning=0.0001 1 iter from common.mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/100-100%20LSTM%20CM_dataset%20songs%20by%20fragments%208-history%207-voices%206-times%2040000%20steps%20lr%3D0.01%20meta_learning%3D0.0001%201%20iter%20from%20common.mid)

Параметром, вообще, у этого дообучения много. В результате уловить разницу не получается; тем не менее, есть два принципиально разных подхода. Примеры выше были построены по принципу "при генерации очередного вектора сбрасываемся к выученным весам, а затем делаем 1 или несколько шагов обучения на сгенерированной последовательности". Альтернатива - обучаться с каждым новым вектором "всё дальше", то есть не сбрасываться к исходным весам на каждом шаге. Learning Rate при этом пришлось выкрутить совсем вниз.

Результат: [100-100 LSTM CM_dataset songs by fragments 8-history 7-voices 6-times 40000 steps lr=0.01 moving_meta_learning=0.00001.mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/100-100%20LSTM%20CM_dataset%20songs%20by%20fragments%208-history%207-voices%206-times%2040000%20steps%20lr%3D0.01%20moving_meta_learning%3D0.00001.mid)

### 16/12 - Capsuled Memory
[LSTM real data capsuled polyphonic one-time model.ipynb](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/LSTM%20real%20data%20capsuled%20polyphonic%20one-time%20model.ipynb)
Ещё одна старая идея. История, т.е. вектора с прошлых моментов, приходят каждый в свою капсулу (однослойный LSTM из всего 24 нейронов), параллельно, затем второй слой из 100 нейронов собирает уже выходы со всех 24x9 капсул. 

Результаты были так себе, пример: [24for9-100 LSTM CM_dataset songs by fragments 8-history 6-times 7-voices (one-time) lr=0.001 15000 epochs.mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/24for9-100%20LSTM%20CM_dataset%20songs%20by%20fragments%208-history%206-times%207-voices%20(one-time)%20lr%3D0.001%2015000%20epochs.mid)

#### 20/12 - теперь с метадообучением!
Примеры в архиве; ситуацию  с этой моделью не изменило.

## 19/12 - HistoryUser
Описание идеи и тестов модели - [в отдельном разделе](https://github.com/FortsAndMills/MusicGeneration/tree/master/HistoryUser%20basic%20tests).

Ноутбук: [LSTM real data HistoryUser polyphonic one-time model.ipynb](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/LSTM%20real%20data%20HistoryUser%20polyphonic%20one-time%20model.ipynb)

На вход подаётся только текущий вектор из 88 элементов. LSTM из 100 нейронов генерит коэффициенты для считывания из истории остальных векторов. Параллельно LSTM из 100 нейронов только по входу обрабатывает входной вектор для определения "локальных" зависимостей. Из обеих капсул выходы собирает трейти LSTM из 100 нейронов и уже генерит фичи для финального линейного преобразования.

Работает: так, при обучении на картинках заметно появление фрагментов, на которых считывание происходит из конкретного места из истории, а лосс на этих фрагментах падает. Тем не менее, при генерации длительное копирование целых фрагментов не наблюдалось, но использование истории ведётся, что лучше, чем ничего, и явно даёт прирост качества, плюс снижает размерность входа. 

Обучение теперь длится уже сутки. Но, набравшись терпения...

## 21/12 - Первые результаты
Качество возросло до уровня "мне уже не настолько стыдно, чтобы не засабмитить". Результат:
[2 Submit. 100-100-100 HistoryUser CM_dataset loss=0.14.mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/2%20%20Submit.%20100-100-100%20HistoryUser%20CM_dataset%20loss%3D0.14.mid)

## 26/12 - Conditional Polyphonic Sampling
Тем не менее, локальные гармонии модель всё ещё улавливает плохо. Причина проста: хочется генерировать вторую ноты в зависимости от засэмплированной первой, а третью - в зависимости от первых двух. Вообще говоря, даже такая модель сэмплирования неадекватна, поскольку речь идёт о генерации ПОДМНОЖЕСТВ 12-элементного множества нот, но с подмножествами нейросети так просто работать не умеют.

Идея соответствующая. Давайте по выходу модели HistoryUser 100-100-100 генерировать вероятность для первой ноты p1 (линейным преобразованием и софтмаксом, как обычно), сэмплировать её, затем на основе ТЕХ ЖЕ выходов модели и сэмплированной ноты ДРУГИМ линейным преобразованием получать вероятность второй ноты p2, сэмплировать её, и т.д. раз 7. При этом градиенты уползут в HistoryUser-модель "семь раз" за каждое линейное преобразование.

Проблема: а как обучать такую модель, если у нас нету разметки о том, на какой из семи итераций какая нота сгенерировалась? Решение придумано моим любимейшим методом глубинного обучения IHNI-WID:
![](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/%D0%A1%D0%B2%D0%B0%D0%BB%D0%BA%D0%B0%20%D1%8D%D0%BA%D1%81%D0%BF%D0%B5%D1%80%D0%B8%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2/I%20have%20no%20idea%20what%20I'm%20doing.jpg)

При обучении обусловим выданную вероятность p1 по правильному ответу (т.е. оставим только возможность засэмплировать те ноты, которые есть в верном ответе) и засэмплируем из него. Таким образом, на вход генератору p2 будет подана именно одна нота (или все нули в случае паузы), причём точно верная, дабы генератор p2 уже обучался в условиях "знания" первой ноты. Далее аналогично.

С реализацией и обучением подобного безумия возникли позже решённые сложности, поэтому код в ноутбуке, LSTM real data polyphonic one-time condttional model.ipynb, представляет собой неудобоваримое месиво...

Однако, был один таинственный момент. Обучаться такая штука не хотела от слова вообще (генерировала 6 пустых голосов, а на 7-ом пыталась генерировать одну правильную ноту) до тех пор, пока я в формуле подсчёта вероятности засэмплировать ноту хотя бы в одном голосе, использующейся в лоссе,
p += (1 - p) * p_i
не остановил градиенты, текущие через (1 - p). Как я до этого дошёл, я и сам не знаю, но это частично ситуацию исправило, однако никакой интерпретации, какого чёрта это помогло, у меня нет, ровно как и лучших идей. Назовём это **"Священный костыль!"**

## 28/12 - Decoder
С самого начала меня прельщало желание работать с 12-мерными векторами вместо 88-мерных, поскольку это на порядок сокращает вычислительную сложность. Однако тут остро стоял открытым вопрос о том, а обратно-то можно ли будет перейти? То есть "закодированная" песня, для каждой из 12 нот известно в каждый момент, играет ли она или нет, но если да, то неизвестно, в какой октаве. Получается, как бы, неизвестно, мелодия ли это или аккомпанемент, грубо упрощая. Ну вроде задача выглядит технически более простой...

Попробовал решить однослойным LSTM-ом. В принципе-то окей, но вот беда - а как же с сэмплированием нескольких нот-то, опять городить "7 голосов" с 7 линейными преобразованиями? Я забил и выдавал 88 вероятностей засэмплировать очередную ноту; те клавиши, о которых в 12-мерном представлении однозначно заявлено, что "нет, они не звучат", насильно зануляются. В том числе и в функции потерь при обучении, конечно же, что позволяет "декодеру" сосредоточиться на запоминании "октавы", в которой происходит игра. Костыльненько, но как-то работает, а главное - с практической точки зрения, ошибки при декодировании приводят к ошибкам на октаву. На слух это не приведёт к нарушению музыкальности звучания, поэтому в рамках получения результата такой метод позволяет сократить 88-мерный вектор к 12-мерному.

## 29/12 - Итоговая Модель
Код вроде аккуратно закомментирован, что особенно существенно в части с Conditional Polyphonic Sampling
[LSTM real data HistoryUser polyphonic one-time model bigger dataset.ipynb](http://localhost:8888/notebooks/Documents/%D0%9C%D0%93%D0%A3/%D0%94%D1%8C%D1%8F%D0%BA%D0%BE%D0%BD%D0%BE%D0%B2%D1%89%D0%B8%D0%BD%D0%B0/RealData%20First%20Model/LSTM%20real%20data%20HistoryUser%20polyphonic%20one-time%20model%20bigger%20dataset.ipynb)

* - HistoryUser + Conditional Polyphonic Sampling + Decoder
* "bigger dataset* - 12-мерные представления циклически рандомно смещаются для получения всех 12 инвариантных представлений одной и той же песенки в выборке. Эту плюшку позволяет получить декодер, без него с 88-мерными представлениями обучение я б не потянул.
* 7 линейных преобразований в Conditional Polyphonic Sampling заменены LSTM-ами (по 40 нейронов), а так как теперь нот всего 12, то количество голосов заменено на 5 вместо 7.

Это удалось обучить, хотя как видно на картинках при обучении, по голосам ноты распределялись всё равно странно. Однако достижение: проявились какие-то гармонии.

Результат: [3 Submit. Conditional Voices and HistoryUser.mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/3%20%20Submit.%20Conditional%20Voices%20and%20HistoryUser.mid)

С мегасуперметакакойбыещёпрефиксдобавитьдообучением: [4  Submit. Conditional Voices and HistoryUser and Metalearning 0.00001.mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/4%20%20Submit.%20Conditional%20Voices%20and%20HistoryUser%20and%20Metalearning%200.00001.mid)

### 02/01 - 05/01 - Борьба с паузами
Что значит "по голосам ноты распределялись странно": судя по картинкам, последующим голосам выгодно повторять то, что засэмплировалось в первых голосах, поскольку в обучении точно известно, что сэмплируется то, что верно. В первом голосе зато, например, проявляются гармонии, то есть генерятся какие-то аккорды, из которых генерится первая нота, что хороший результат. Однако, а если генерится пауза? Ведь тогда последующим голосам "не на что опереться", и они могут тоже радостно засэмплировать паузу. Странности модели, так сказать. Было несколько не менее странных способов решения этой проблемы, но успеха они не принесли.

### 06/01 - Pause Loss
Вот какая мысль вроде "как-то" сработала: мы работаем с 89-мерными распределениями, а в лоссе распределения 88-мерные. Это из-за 89-ой "паузы", вероятность которой в лоссе как бы не учитывается. А может, надо учесть? Если все пять голосов выдали именно её, то считаем, что мы сгенерировали "паузу", которая по всё той же родимой кросс-энтропии сравнивается с тем, а действительно ли в песне была пауза (отсутствовали ноты).

Я бы не сильно заинтересовался такой модификацией, если бы не одно но: это позволило убрать самое больное место модели в виде того самого Священного Костыля. Пустив градиенты по (1 - p) с дополнительными потерями на паузах почему-то в одноголосие ничего не вырождалось.

Код: [LSTM real data HistoryUser polyphonic one-time model bigger dataset pause loss.ipynb](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/LSTM%20real%20data%20HistoryUser%20polyphonic%20one-time%20model%20bigger%20dataset%20pause%20loss.ipynb)

Результат: [5 Submit. Conditional Voices and HistoryUser Correct Pause-loss.mid](https://github.com/FortsAndMills/MusicGeneration/blob/master/RealData%20First%20Model/5%20%20Submit.%20Conditional%20Voices%20and%20HistoryUser%20Correct%20Pause-loss.mid)

## Основные выводы.
* Выбранный датасет неадекватен.
* Выбранное представление данных неадекватно.
* Выбранная модель памяти неадекватна.
* Выбранный способ полифонического сэмплирования неадекватен.
* Как-то работает.
