# PyTorch Experiments

## Baseline Model

Исходной моделью здесь будем считать результат [экспериментов в тензорфлоу](https://github.com/FortsAndMills/MusicGeneration/tree/master/RealData%20First%20Model), а именно:
* сжатие в 12-мерное представление и последующее декодирование обычным однослойным LSTM-ом с предположением о независимости ответа для всех клавиш при декодировке.
* схема 100-100 HistoryUser для дополнительного использования истории
* полученные высокоуровневые фичи отправляются в полифонический сэмплер, который работает по следующему принципу:
    * является однослойным LSTM-ом.
    * выдаёт 24 вероятности, интерпретируемых как категориальное распределение на следующем домене: 12 исходов "нота звучит" и 12 исходов "нота точно не звучит".
    * сэмплер генерирует это распределение, из него происходит сэмплирование, результат подаётся сэмплеру на вход и он снова генерирует распределение; и так 5 раз.
    * на выходе LSTM-а вероятности засэмплировать исход, относящейся к ноте, по которой решение уже принято, насильно зануляются. Таким образом, по каждой ноте принимается одно окончательное решение.
    * обучение происходит по no aposter схеме, а именно: если при обучении сэмплируется по некоторой ноте неверное решение, оно инвертируется. Таким образом, при обучении каждый "голос" получает на вход только верную информацию.
    * вероятности, выданные всеми 5 голосами, с учётом занулений, используются для вычислений следующих вероятностей:
        * вероятность того, что нота будет забанена b = b1 + (1 - b1)b2 + (1 - b1)(1 - b2)b3 + ...
        * вероятность того, что нота будет звучать p = p1 + (1 - p1)p2 + (1 - p1)(1 - p2)p3 + ...
    * тогда вероятность того, что будет сгенерирована 1 для i-ой ноты, равна p(1 - b).
    * минимизируется кросс-энтропия.
    
Реализация: PyTorch Baseline Model.
Модель для CP датасета: Baseline.pt, Baseline Decoder.pt

### Исследуемые вопросы:
* Можно ли заменить декодер на LSTM-RBM?
* Можно ли генерировать громкость нот?
* Можно ли улучшить attention (т.е. HistoryUser-схему), заменив её на, например, иерархическую сеть?
* Станет ли лучше при замене представления с текущего 1 = нота нажимается на 1 = нота звучит?

## Decoder LSTM-RBM
Теоретически RBM на выходе декодера смотрится логичнее. На практике, распределение x_{t + 1} ~ p(x | h0), где h0 ~ p(h | x_t) в результате обучения оказывается достаточно вырожденным. Сильно лучше по сравнению с "тупой" моделью не стало, и есть места, где этот декодер откровенно подводит, но вырожденное распределение выглядит логичным в отличии от распределения "тупой" модели.

Немного, только, подозрительно выглядит график обучения (поскольку RBM - "энергетическая" модель, там минимизируется т.н. свободная энергия), и ведёт она себя странно - дисперсия большая, с ходом обучения не сильно-то и уменьшается, если даже и не повышается, но картинки распределения по ходу обучения говорят, что процесс движется, и в верном направлении, поэтому, возможно, я просто никогда не работал с подобными моделями.

## Velocity Generator
Сначала затупил и объединил с декодером в одну сеть, смешав лоссы, не удосужившись их совместно отмасштабировать - после исправления, однако, результат получился странный. Распределение мало зависит от времени; верхним нотам выдаёт чуть бОльшую громкость, чем нижним, за счёт чего выделяет "типа-мелодию" из верхних нот, что наблюдается и в данных, но сильно аккуратнее. Звучать лучше не стало - громкость зачастую занижена, а поскольку мелодия не генерируется "специально", выделение верхних нот создаёт очень странное ощущение. В общем, эта штука под вопросом.

## Замена представления (на holding-представление, 1 = нота звучит, не обязательно нажимается конкретно в этот момент)
Похоже, что в моём случае текущее представление лучше: причина в декодере. Если одна нота - это последовательность единиц, то одни и те же ноты в разных октавах сливаются. На плечи декодера взваливается довольно странная задача - разделить 11111111 на 11111111 в, скажем, октаве №1, и 00001111 в октаве, скажем, №2. Декодеру и сейчас приходится из одной ноты в зашифрованном представлении создавать несколько одинаковых нот в разных октавах, но это происходит в один момент времени, и для этого-то как раз RBM на его выходе и появился. А вот с таким представлением появляются странные режущие слух артефакты. Предлагается оставить всё как было.

## Hierarchical models
А вот это самое интересное. Иерархические сети на пайторче выдали совсем плохое правдоподобие и соответствующе звучат, как при 7 (не очень адекватное число согласно статьям по этим сетям, где максимум 3 используют) уровнях абстрактности, так и при 3, но появилась возможность "повторить" другой эксперимент на пайторче - LatencyLSTM

LatencyLSTM - когда рекуррентные связи задерживают информацию не на t=1, а на t=128, например. Поскольку фрагменты песен, которые я использую при обучении, имеют продолжительность 256, полагаю, что ну наверное, как-то обучаться такие слои должны. Использовалась связка нейронов с разными задержками (1, 2, 4, 8, 16, 32, 48, 64, 96, 128) в одном как бы слое сети, который заменял всю HistoryUser-схему за исключением аггрегирующего слоя. Иначе говоря, Latency-слой передавал данные обычному слою LSTM, а дальше уже полифонический сэмплер.

Вот такая схема работает, но сэмплы раз на раз не приходятся и словно стали менее разнообразны. Иерархической структуры не появилось ;), но было подозрение на появление какой-то локальной структуры мелодий, то, чего модели сейчас сильно не хватает - у неё и с локальной-то структурой проблемы. Как сравнить LatencyLSTM и HistoryUser - не знаю, апрель месяц, идёт дождь.
